# ðŸŽ¯ Task 01.2: Language Detection

## ðŸ“… Sprint Info
- **Duration:** 4 days
- **Milestone:** M0 - Foundation
- **Priority:** P0 (Critical Path)
- **Depends On:** Task 01.1 (Unicode Normalization) âœ…
- **Blocks:** Task 01.3 (Script Processing)
- **Assignee:** TBD

## ðŸŽ¬ What You'll Build
Build universal automatic language detection supporting 100+ languages without manual configuration. Uses n-gram analysis to identify language with confidence scoring.

## ðŸ“‹ Daily Breakdown

### Day 1: Setup & N-gram Model
- [ ] Review Task 01.1 normalized text output
- [ ] Install langdetect, polyglot libraries
- [ ] Implement n-gram extraction (1-3 grams)
- [ ] Build language profile database
- [ ] Initial tests with 10 languages

### Day 2: Multi-language Support
- [ ] Expand to 50+ languages
- [ ] Implement confidence scoring (0.0-1.0)
- [ ] Add mixed-content detection
- [ ] Handle short text edge cases (<50 chars)
- [ ] Add fallback mechanisms

### Day 3: Script Detection & Integration
- [ ] Add ISO 15924 script detection
- [ ] Integrate with normalized text from Task 01.1
- [ ] Build LanguageInfo struct
- [ ] API endpoint design
- [ ] Integration tests with normalizer

### Day 4: Optimization & Testing
- [ ] Performance optimization (target: <5ms/query)
- [ ] Expand test corpus to 100+ languages
- [ ] Accuracy validation (â‰¥95% target)
- [ ] Edge case handling (empty, very short, mixed)
- [ ] Documentation and code review

## âœ… Acceptance Criteria
- [ ] Language detection accuracy â‰¥95% on test corpus
- [ ] Support 100+ languages automatically
- [ ] Confidence scoring for reliability
- [ ] Handle mixed-language content
- [ ] Detection latency <5ms per document
- [ ] Script detection (ISO 15924) included
- [ ] Unit test coverage â‰¥85%

## ðŸ§ª Testing & Validation

### Test Corpus
```python
test_samples = {
    "en": "The quick brown fox jumps over the lazy dog",
    "fa": "Ø§ÛŒÙ† ÛŒÚ© Ù…ØªÙ† Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ø§Ø³Øª",
    "ar": "Ù‡Ø°Ø§ Ù†Øµ ØªØ¬Ø±ÙŠØ¨ÙŠ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©",
    "zh": "è¿™æ˜¯ä¸€ä¸ªä¸­æ–‡ç¤ºä¾‹æ–‡æœ¬",
    "mixed": "Hello Ø³Ù„Ø§Ù… ä½ å¥½ ÐŸÑ€Ð¸Ð²ÐµÑ‚"  # Multi-language
}
```

### Accuracy Test
```bash
python -m pytest tests/test_language_detector.py
# Expected: â‰¥95% accuracy across 50+ languages
```

## ðŸŽ‰ Celebration Criteria (Definition of Done)
âœ… **Demo Ready:** Live detection of 10 languages with confidence scores  
âœ… **95% Accuracy:** Validated on diverse test corpus  
âœ… **Fast:** <5ms detection time  
âœ… **Mixed Content:** Correctly identifies multi-language text  
âœ… **Integration:** Works seamlessly with Task 01.1 output  

**ðŸŽŠ Celebration Moment:** Demo real-time language detection with mixed text!

## ðŸ“¦ Deliverables
- `src/python/text_processor/language_detector.py` (200-250 lines)
- `src/python/text_processor/ngram_model.py` (100-150 lines)
- `tests/test_language_detector.py` (150+ test cases)
- `data/language_profiles/` (n-gram profiles for 100+ languages)
- `docs/api/language-detection.md`

## ðŸ”— Dependencies & Integration

### Input (from Task 01.1)
```python
normalized_text: str  # From normalizer
```

### Output
```python
class LanguageInfo:
    language_code: str    # ISO 639-1 (e.g., "fa", "en", "zh")
    script_code: str      # ISO 15924 (e.g., "Arab", "Latn", "Hani")
    confidence: float     # 0.0-1.0 confidence score
    is_mixed_content: bool  # True if multiple languages detected
    detected_languages: List[Tuple[str, float]]  # All detected with scores
```

## ðŸš€ Next Steps
âž¡ï¸ **Task 01.3: Script-Specific Processing** (5 days)  
- Will use language_code to apply script-specific rules
- Depends on accurate language identification

## ðŸ’¡ Tips & Resources

### Common Pitfalls
- âš ï¸ Short text (<50 chars) needs special handling
- âš ï¸ Similar scripts (Persian vs Arabic) need careful tuning
- âš ï¸ Mixed-language content is common in search queries
- âš ï¸ Don't rely on single library - use ensemble approach

### Helpful Resources
- [langdetect library](https://github.com/Mimino666/langdetect)
- [polyglot documentation](https://polyglot.readthedocs.io/)
- [ISO 639 Language Codes](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes)
- [ISO 15924 Script Codes](https://unicode.org/iso15924/)

### Example Code
```python
from langdetect import detect_langs

def detect_language_universal(text: str) -> LanguageInfo:
    """Universal language detection with confidence"""
    results = detect_langs(text)
    
    return LanguageInfo(
        language_code=results[0].lang,
        confidence=results[0].prob,
        script_code=detect_script(text),
        is_mixed_content=len(results) > 1
    )
```

## ðŸ“Š Success Metrics
- **Accuracy:** â‰¥95% on diverse test corpus
- **Performance:** <5ms per document
- **Coverage:** 100+ languages supported
- **Reliability:** Handles edge cases gracefully

## ðŸŽ“ Learning Outcomes
After completing this task, you will:
- âœ… Understand n-gram language models
- âœ… Know script detection techniques
- âœ… Handle multi-language content
- âœ… Build confidence-based classification systems

## ðŸ› Known Edge Cases
- Very short text (<10 chars): Use fallback heuristics
- All-numeric text: Return "unknown" with low confidence
- Mixed scripts: Return primary + secondary languages
- Transliterated text: May misclassify (acceptable limitation)

---

**Let's detect every language in the world! ðŸŒ**

