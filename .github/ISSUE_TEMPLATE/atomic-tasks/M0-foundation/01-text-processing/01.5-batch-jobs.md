# ðŸŽ¯ Task 01.5: Nightly Batch Jobs

## ðŸ“… Sprint Info
- **Duration:** 3 days
- **Milestone:** M0 - Foundation
- **Priority:** P1 (Important)
- **Depends On:** Task 01.4 (Stopword IDF Analysis) âœ…
- **Blocks:** None (Parallel with 01.6)
- **Assignee:** TBD

## ðŸŽ¬ What You'll Build
Create automated nightly batch jobs to refresh stopword lexicon from corpus updates. Ensures stopwords stay current as content changes.

## ðŸ“‹ Daily Breakdown

### Day 1: Job Infrastructure
- [ ] Design job scheduler architecture
- [ ] Setup cron/Airflow/custom scheduler
- [ ] Build job configuration system
- [ ] Implement job logging and monitoring
- [ ] Create job status API

### Day 2: Stopword Refresh Pipeline
- [ ] Integrate with Task 01.4 IDF analyzer
- [ ] Build incremental corpus reader
- [ ] Implement delta updates (only changed docs)
- [ ] Export updated stopwords to Redis
- [ ] Add rollback mechanism for failures

### Day 3: Monitoring & Testing
- [ ] Setup alerting for job failures
- [ ] Performance optimization (SLA: <1 hour for 100M docs)
- [ ] Test job execution end-to-end
- [ ] Documentation for ops team
- [ ] Code review

## âœ… Acceptance Criteria
- [ ] Nightly job runs automatically at configured time
- [ ] Processes 100M+ documents within 1 hour
- [ ] Updates Redis stopword cache atomically
- [ ] Sends alerts on failures
- [ ] Logs execution metrics (duration, docs processed, errors)
- [ ] Rollback on failures (previous stopwords remain)
- [ ] Manual trigger capability for emergency updates

## ðŸ§ª Testing & Validation

### Job Execution Test
```bash
# Manual trigger
python -m batch_jobs.stopword_refresh --dry-run

# Expected output:
# âœ“ Connected to MongoDB
# âœ“ Processed 100M documents in 45 minutes
# âœ“ Found 150K stopword candidates across 50 languages
# âœ“ Exported to Redis successfully
```

### Performance Test
```python
# Target: 100M docs in <1 hour
# That's ~28K docs/second
# Parallelization required!
```

## ðŸŽ‰ Celebration Criteria (Definition of Done)
âœ… **Auto-Runs:** Job executes nightly without manual intervention  
âœ… **Fast:** Completes within 1 hour for 100M docs  
âœ… **Reliable:** Rollback works on failures  
âœ… **Monitored:** Alerts sent to ops team  
âœ… **Documented:** Runbook ready for production  

**ðŸŽŠ Celebration Moment:** Watch first successful nightly execution!

## ðŸ“¦ Deliverables
- `src/python/batch/nightly_stopword_refresh.py` (300 lines)
- `src/python/batch/job_scheduler.py` (200 lines)
- `src/python/batch/monitoring.py` (150 lines)
- `config/batch_jobs.yaml` (job configuration)
- `docs/ops/stopword-refresh-runbook.md`
- `tests/test_batch_jobs.py` (50+ test cases)

## ðŸ”— Dependencies & Integration

### Input
```python
# From MongoDB
updated_documents: Iterator[Document]  # Only changed docs since last run

# From Task 01.4
idf_analyzer: IDFAnalyzer  # Reuse IDF calculation logic
```

### Output
```python
class JobResult:
    success: bool
    duration_seconds: float
    documents_processed: int
    stopwords_updated: int
    errors: List[str]
    timestamp: datetime
```

### Integration Points
- MongoDB: Read corpus updates
- Redis: Atomic stopword cache update
- Monitoring: Send metrics to Grafana/Prometheus
- Alerts: Email/Slack on failures

## ðŸš€ Next Steps
âž¡ï¸ **Task 01.6: C++ Integration** (4 days)  
- Parallel task - can start immediately

âž¡ï¸ **Production Deployment**  
- After M0 complete, schedule in production

## ðŸ’¡ Tips & Resources

### Common Pitfalls
- âš ï¸ **Atomic updates:** Use Redis MULTI/EXEC for transactional updates
- âš ï¸ **Long-running jobs:** Implement checkpointing for resume
- âš ï¸ **Resource limits:** Don't overload MongoDB with full scans
- âš ï¸ **Timezone issues:** Use UTC for all scheduling

### Helpful Resources
- [APScheduler](https://apscheduler.readthedocs.io/) (Python job scheduling)
- [Airflow](https://airflow.apache.org/) (enterprise job orchestration)
- [Redis Transactions](https://redis.io/docs/manual/transactions/)
- [MongoDB Change Streams](https://www.mongodb.com/docs/manual/changeStreams/)

### Example Code
```python
from apscheduler.schedulers.blocking import BlockingScheduler

def refresh_stopwords():
    """Nightly stopword refresh job"""
    try:
        # Read updated docs from MongoDB
        updated_docs = get_updated_documents(since=last_run_time)
        
        # Recalculate IDF
        idf_analyzer = IDFAnalyzer()
        new_stopwords = idf_analyzer.analyze(updated_docs)
        
        # Atomic Redis update
        with redis.pipeline() as pipe:
            pipe.delete("stopwords:*")
            for lang, words in new_stopwords.items():
                for word, confidence in words:
                    pipe.hset(f"stopwords:{lang}", word, confidence)
            pipe.execute()
        
        log_success("Stopword refresh completed")
    
    except Exception as e:
        log_error(f"Job failed: {e}")
        send_alert("Stopword refresh failed!")
        raise

# Schedule for 2 AM UTC daily
scheduler = BlockingScheduler()
scheduler.add_job(refresh_stopwords, 'cron', hour=2)
scheduler.start()
```

## ðŸ“Š Success Metrics
- **Reliability:** 99%+ successful executions
- **Performance:** <1 hour for 100M docs
- **Freshness:** Stopwords updated within 24 hours
- **MTTR:** <15 minutes from alert to fix

## ðŸŽ“ Learning Outcomes
After completing this task, you will:
- âœ… Build production batch job systems
- âœ… Implement job monitoring and alerting
- âœ… Handle long-running data processing
- âœ… Design fault-tolerant pipelines

## ðŸ”§ Operations Checklist
- [ ] Job runs in production environment
- [ ] Monitoring dashboard setup
- [ ] Alert channels configured
- [ ] Runbook documented
- [ ] Rollback procedure tested
- [ ] On-call team trained

## ðŸ“… Scheduling Options
1. **Simple:** Linux cron (good for MVP)
2. **Intermediate:** APScheduler (Python-native)
3. **Enterprise:** Apache Airflow (full orchestration)

---

**Automate all the things! ðŸ¤–**

