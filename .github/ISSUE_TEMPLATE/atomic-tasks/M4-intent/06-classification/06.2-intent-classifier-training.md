# ðŸŽ¯ Task 06.2: Intent Classifier Training (Info/Trans/Nav)

## ðŸ“… Sprint Info
- **Duration:** 4 days
- **Milestone:** M4 - Intent & Vertical Classification
- **Priority:** P1
- **Depends On:** Task 06.1 (Weak Label Generation) âœ…
- **Blocks:** Task 06.4 (Model Integration)
- **Assignee:** TBD

## ðŸŽ¬ What You'll Build
Train gradient boosting classifier (XGBoost/LightGBM) for query intent detection (Informational/Transactional/Navigational) using weak labels. Works across any language automatically with feature engineering from query and document signals.

## ðŸ“‹ Daily Breakdown

### Day 1: Feature Engineering
- [ ] Extract query features (length, word count, patterns)
- [ ] Build commercial keyword features (multilingual)
- [ ] Add brand/domain matching features
- [ ] Extract document features (schema type, price presence)
- [ ] Create query-document interaction features
- [ ] Add language-agnostic features
- [ ] Feature importance analysis

### Day 2: Model Training
- [ ] Implement GBDT classifier (XGBoost/LightGBM)
- [ ] Build training pipeline with weak labels
- [ ] Add class balancing (SMOTE/class weights)
- [ ] Hyperparameter tuning (grid search)
- [ ] Cross-validation (5-fold)
- [ ] Model evaluation on validation set

### Day 3: Multi-language Validation
- [ ] Test on 10+ languages
- [ ] Per-language performance analysis
- [ ] Feature adaptation for new languages
- [ ] Ensemble methods (if needed)
- [ ] Confidence calibration
- [ ] Error analysis

### Day 4: Model Export & Documentation
- [ ] Model serialization (pickle/ONNX)
- [ ] Create prediction API
- [ ] Performance benchmarking (<2ms inference)
- [ ] Documentation and examples
- [ ] Model versioning

## âœ… Acceptance Criteria
- [ ] Intent classification accuracy â‰¥75% on test set
- [ ] Works across 10+ languages automatically
- [ ] Navigational precision â‰¥80%
- [ ] Transactional precision â‰¥75%
- [ ] Inference latency <2ms per query
- [ ] Model size <100MB
- [ ] Confidence scores calibrated

## ðŸ§ª Testing Checklist
- [ ] Unit tests for feature extraction (30+ cases)
- [ ] Model accuracy tests per intent class
- [ ] Multi-language validation tests
- [ ] Performance benchmarks (<2ms)
- [ ] Edge case tests (short queries, mixed language)
- [ ] Confidence calibration tests

## ðŸŽ‰ Celebration Criteria (Definition of Done)
âœ… **Demo Ready:** Show intent predictions on diverse queries across languages
âœ… **Metric Met:** 75% accuracy, <2ms latency
âœ… **Integration:** Model ready for production deployment
âœ… **Coverage:** Works for 10+ languages

**ðŸŽŠ Celebration Moment:** AI understands user intent automatically!

## ðŸ“¦ Deliverables
- `src/python/intent_classifier/feature_extractor.py` (400 lines)
- `src/python/intent_classifier/trainer.py` (350 lines)
- `src/python/intent_classifier/predictor.py` (200 lines)
- `models/intent_classifier_v1.pkl` (trained model)
- `tests/test_intent_classifier.py` (50+ test cases)
- `docs/intent-classification-guide.md`

## ðŸ”— Dependencies & Integration

### Input Features
```python
# Query features
{
    "query_length": int,
    "word_count": int,
    "has_brand": bool,
    "has_price_keyword": bool,
    "has_how_to": bool,
    "has_what_is": bool,
    "commercial_score": float,
    "language": str
}

# Document features
{
    "schema_type": str,
    "has_price": bool,
    "has_offer": bool,
    "is_homepage": bool,
    "host_rank": float,
    "url_depth": int
}
```

### Output
```python
# Intent prediction
{
    "intent": str,  # "informational", "transactional", "navigational"
    "confidence": float,  # 0.0-1.0
    "probabilities": {
        "informational": float,
        "transactional": float,
        "navigational": float
    },
    "model_version": str
}
```

### Example Code

```python
import xgboost as xgb
import numpy as np
from typing import Dict, List

class IntentClassifier:
    def __init__(self):
        self.model = None
        self.feature_extractor = FeatureExtractor()
        
    def train(self, training_data: List[Dict]):
        """Train intent classifier on weak labels"""
        
        # Extract features
        X, y = self.prepare_training_data(training_data)
        
        # Handle class imbalance
        scale_pos_weight = self.compute_class_weights(y)
        
        # Train XGBoost
        self.model = xgb.XGBClassifier(
            max_depth=6,
            learning_rate=0.1,
            n_estimators=100,
            objective='multi:softprob',
            scale_pos_weight=scale_pos_weight,
            random_state=42
        )
        
        self.model.fit(X, y)
        
        return self.model
    
    def predict(self, query: str, doc: Dict) -> Dict:
        """Predict intent for query-document pair"""
        
        # Extract features
        features = self.feature_extractor.extract(query, doc)
        X = np.array([features])
        
        # Predict probabilities
        probs = self.model.predict_proba(X)[0]
        
        # Get predicted class
        intent_idx = np.argmax(probs)
        intent_labels = ['informational', 'transactional', 'navigational']
        intent = intent_labels[intent_idx]
        
        return {
            'intent': intent,
            'confidence': float(probs[intent_idx]),
            'probabilities': {
                'informational': float(probs[0]),
                'transactional': float(probs[1]),
                'navigational': float(probs[2])
            },
            'model_version': 'v1'
        }
    
    def prepare_training_data(self, data: List[Dict]):
        """Prepare X, y for training"""
        X = []
        y = []
        
        intent_to_idx = {
            'informational': 0,
            'transactional': 1,
            'navigational': 2
        }
        
        for item in data:
            features = self.feature_extractor.extract(
                item['query'],
                item['doc']
            )
            X.append(features)
            y.append(intent_to_idx[item['intent_label']])
        
        return np.array(X), np.array(y)


class FeatureExtractor:
    def __init__(self):
        self.commercial_keywords = self.load_commercial_keywords()
        self.brand_list = self.load_brands()
        
    def extract(self, query: str, doc: Dict) -> List[float]:
        """Extract features for classification"""
        
        features = []
        
        # Query features
        features.append(len(query))  # Query length
        features.append(len(query.split()))  # Word count
        features.append(self.has_brand(query))  # Brand presence
        features.append(self.commercial_score(query))  # Commercial score
        features.append(self.has_how_to_pattern(query))  # How-to query
        features.append(self.has_what_is_pattern(query))  # What is query
        
        # Document features
        features.append(doc.get('has_price', 0))
        features.append(doc.get('has_offer', 0))
        features.append(self.is_homepage(doc.get('url', '')))
        features.append(doc.get('host_rank', 0.0))
        features.append(self.get_url_depth(doc.get('url', '')))
        
        # Interaction features
        features.append(self.query_matches_domain(query, doc.get('url', '')))
        
        return features
    
    def commercial_score(self, query: str) -> float:
        """Score commercial intent in query"""
        score = 0.0
        query_lower = query.lower()
        
        for keyword in self.commercial_keywords:
            if keyword in query_lower:
                score += 0.2
        
        return min(score, 1.0)
    
    def has_brand(self, query: str) -> int:
        """Check if query contains brand name"""
        query_lower = query.lower()
        return int(any(brand in query_lower for brand in self.brand_list))
```

## ðŸ“Š Success Metrics
- **Overall Accuracy:** â‰¥75%
- **Navigational Precision:** â‰¥80%
- **Transactional Precision:** â‰¥75%
- **Informational Recall:** â‰¥70%
- **Inference Latency:** <2ms
- **Model Size:** <100MB
- **Multi-language:** Works for 10+ languages

## ðŸŽ“ Learning Outcomes
- âœ… Train classifiers with weak supervision
- âœ… Feature engineering for NLP tasks
- âœ… Handle class imbalance in ML
- âœ… Deploy gradient boosting models
- âœ… Build language-agnostic classifiers

---

**Understanding intent, serving results! ðŸŽ¯ðŸ¤–**

