---
name: "15.3 - Feature Glossary & Ranking Signal Documentation"
about: Comprehensive documentation of all ranking features, signals, and scoring formulas
title: "[M9][documentation] Feature glossary for all ranking signals"
labels: kind/feature, area/documentation, priority/P1, status/backlog
assignees: ''
milestone: M9-production
estimated_duration: 4-5 days
dependencies:
  - Ranking fusion implemented (08-ranking-fusion)
  - All feature extraction complete (M0-M6)
  - FinalScore formula finalized
---

## Summary
Create comprehensive documentation for all ranking features and signals used in the universal multilingual search engine. This includes mathematical formulas, implementation details, parameter values, feature weights, and practical examples for understanding how each signal contributes to the final ranking score.

## Context
Feature glossary is essential for:
- Understanding ranking behavior and debugging issues
- Training new team members on ranking logic
- Explaining search results to stakeholders
- Parameter tuning and optimization decisions
- Feature importance analysis and ablation studies

## Goals
- **Primary:** Document all ranking features with formulas and examples
- **Secondary:** Explain feature weight rationale and tuning history
- **Stretch:** Interactive feature calculator/simulator

## Tasks

### Core Ranking Features

- [ ] **BM25 Score (Lexical Relevance)**
  ```markdown
  ## BM25 (Best Match 25)
  
  ### Description
  Lexical relevance score based on term frequency (TF) and inverse document frequency (IDF).
  Foundation of retrieval, works universally across all languages.
  
  ### Formula
  BM25(d|q) = Î£ (IDF(qi) Ã— (f(qi,d) Ã— (k1 + 1)) / (f(qi,d) + k1 Ã— (1 - b + b Ã— |d|/avgdl)))
  
  Where:
  - qi: Query term i
  - f(qi,d): Frequency of qi in document d
  - |d|: Document length (number of terms)
  - avgdl: Average document length in corpus
  - k1: Term saturation parameter (default: 1.2)
  - b: Length normalization parameter (default: 0.75)
  - IDF(qi) = log((N - df(qi) + 0.5) / (df(qi) + 0.5))
  - N: Total number of documents
  - df(qi): Number of documents containing qi
  
  ### Field Weights (Universal)
  - title: 3.0 (highest importance)
  - h1h3: 2.0 (section headings)
  - body: 1.0 (base content)
  - anchors: 2.5 (anchor text from links)
  - url: 1.5 (URL tokens)
  
  ### Range
  Typically 0-100+, but can be higher for highly relevant documents.
  Most results score between 5-30.
  
  ### Language Handling
  - Universal: Works for any language/script after Unicode normalization
  - Stopwords: Automatically detected via IDF analysis (language-agnostic)
  - N-grams: Character 3-5 grams as fallback for unknown terms
  
  ### Implementation
  Location: `src/storage/SearchStorage.cpp` (via RedisSearch/MongoDB)
  
  ### Example
  Query: "machine learning tutorial"
  Document 1: Title="Machine Learning Tutorial", Body="Complete guide..."
    â†’ BM25 â‰ˆ 25.3 (exact title match + body occurrence)
  
  Document 2: Title="Introduction to AI", Body="...machine learning..."
    â†’ BM25 â‰ˆ 8.7 (partial body match only)
  
  ### Tuning History
  - Initial k1=1.5, b=0.75 (Elasticsearch defaults)
  - Adjusted k1=1.2 after corpus analysis (shorter docs)
  - Field weights tuned via proxy metrics optimization
  ```

- [ ] **Embedding Similarity (Semantic Relevance)**
  ```markdown
  ## EmbSim (Embedding Similarity)
  
  ### Description
  Semantic similarity between query and document using multilingual subword embeddings.
  Captures meaning beyond exact keyword matches.
  
  ### Formula
  EmbSim(q, d) = cosine_similarity(embed(q), embed(d))
                = (embed(q) Â· embed(d)) / (||embed(q)|| Ã— ||embed(d)||)
  
  Where:
  - embed(q): Query embedding vector (300-dim)
  - embed(d): Document embedding vector (300-dim, precomputed)
  - Cosine similarity range: [-1, 1], typically [0, 1] for relevant results
  
  ### Embedding Model
  - Architecture: Subword skip-gram (fastText-style)
  - Training: Co-occurrence matrix â†’ PPMI â†’ Truncated SVD
  - Dimensions: 300
  - Vocabulary: 10M+ terms (multilingual)
  - Subword n-grams: 3-6 characters (handles OOV terms)
  
  ### Document Embedding
  - Precomputed: Title (70%) + Body snippet (30%) weighted average
  - Updated: Weekly batch job (incremental for new docs)
  - Storage: Feature store (MongoDB) with vector IDs
  
  ### Query Embedding
  - Real-time: Computed on-the-fly with caching
  - Cache: Redis (TTL 1 hour for frequent queries)
  - Latency: <5ms (cached), <15ms (cold)
  
  ### Range
  0.0 - 1.0 (cosine similarity)
  - 0.9-1.0: Very high semantic similarity (near-synonyms)
  - 0.7-0.9: Strong semantic match
  - 0.5-0.7: Moderate relevance
  - <0.5: Weak or no semantic relation
  
  ### Language Handling
  - Universal: Multilingual embeddings via cross-lingual alignment
  - Cross-language: Can match Persian query to English doc (lower score)
  - Script-agnostic: Works for Latin, Arabic, Cyrillic, CJK, etc.
  
  ### Implementation
  Location: `src/services/EmbeddingService.cpp`, `05-embeddings/`
  
  ### Example
  Query: "deep learning frameworks"
  Document 1: "TensorFlow and PyTorch comparison"
    â†’ EmbSim â‰ˆ 0.83 (high semantic similarity, different words)
  
  Document 2: "Deep learning frameworks overview"
    â†’ EmbSim â‰ˆ 0.91 (very high, overlapping terms + semantics)
  
  ### Tuning History
  - Initial: 200-dim embeddings, similarity â‰ˆ 0.15 weight
  - Improved: 300-dim + PPMI preprocessing â†’ 0.15 weight in FinalScore
  - Current: Subword embeddings handle OOV better, multilingual coverage
  ```

- [ ] **HostRank (Domain Authority)**
  ```markdown
  ## HostRank (Host-Level PageRank)
  
  ### Description
  Authority score for the host domain based on link graph analysis.
  Universal measure of site reputation across all languages.
  
  ### Formula
  HostRank(d) = PageRank(host(d))
  
  PageRank: PR(h) = (1-d)/N + d Ã— Î£ (PR(hi) / L(hi))
  
  Where:
  - h: Host domain (eTLD+1)
  - d: Damping factor (0.85)
  - N: Total number of hosts
  - hi: Inlinking host i
  - L(hi): Number of outlinks from host hi
  - Normalized to [0, 1] range
  
  ### Computation
  - Graph: Host-level (collapsed by eTLD+1)
  - Algorithm: Power iteration (20-30 iterations)
  - Frequency: Daily incremental updates
  - Initialization: Uniform distribution (1/N)
  
  ### Range
  0.0 - 1.0 (normalized)
  - 0.8-1.0: Top-tier sites (Wikipedia, major news, .edu/.gov)
  - 0.5-0.8: Established sites with good authority
  - 0.3-0.5: Mid-tier sites
  - 0.1-0.3: Smaller sites
  - <0.1: New or low-authority sites
  
  ### Language Handling
  - Universal: Language-independent link analysis
  - Cross-language: Links between different language sites counted
  - Script-agnostic: Works for all domains regardless of content language
  
  ### Implementation
  Location: `03-link-graph/03.3-pagerank-computation.md`
  Storage: Feature store (MongoDB `hostrank_norm` field)
  
  ### Example
  Query: "quantum computing"
  - wikipedia.org â†’ HostRank â‰ˆ 0.95 (very high authority)
  - mit.edu â†’ HostRank â‰ˆ 0.88 (high authority .edu)
  - quantummagazine.com â†’ HostRank â‰ˆ 0.72 (established tech site)
  - personal-blog.com â†’ HostRank â‰ˆ 0.15 (low authority)
  
  ### Tuning History
  - Initial: Basic PageRank with d=0.85, weight=0.10 in FinalScore
  - Current: Host-level aggregation (faster, more stable than URL-level)
  - Future: Incorporate topic-specific authority (e.g., higher weight for .edu on academic queries)
  ```

- [ ] **Anchor Match (Anchor Text Relevance)**
  - Mathematical formula
  - Anchor text extraction method
  - Scoring mechanism
  - Weight in FinalScore
  - Examples

- [ ] **Structured Boost (Schema.org Bonus)**
  - Structured data types (Book, Product, Article)
  - Boost multipliers by type
  - Detection method (JSON-LD, Microdata, RDFa)
  - Query-dependent boosting logic
  - Examples

- [ ] **Freshness Decay (Recency Factor)**
  - Time decay formula (exponential)
  - Half-life parameter
  - Timestamp source (publication date, crawl date, update date)
  - Query-dependent freshness (news vs. evergreen)
  - Examples

- [ ] **URL Quality (URL Pattern Heuristics)**
  - Quality signals (URL length, depth, parameters)
  - Penalty factors (too long, too many params, suspicious patterns)
  - Boost factors (clean URLs, readable paths)
  - Range and normalization
  - Examples

- [ ] **Spamness (Spam Probability)**
  - Spam detection model (One-Class SVM / Isolation Forest)
  - Feature extraction (text/HTML ratio, ad density, keyword abuse)
  - Score calibration [0, 1]
  - Penalty application in FinalScore
  - Site-level aggregation
  - Examples

- [ ] **Intent Align (Query-Document Intent Match)**
  - Intent classification (Informational, Transactional, Navigational)
  - Document intent detection (weak supervision)
  - Alignment scoring
  - Boost for intent match
  - Examples

### Final Score Formula

- [ ] **FinalScore Computation**
  ```markdown
  ## FinalScore (Ranking Formula)
  
  ### Description
  Weighted combination of all ranking features to produce final document score.
  
  ### Formula
  FinalScore(d|q) = 
      0.55 Ã— BM25(d|q)
    + 0.15 Ã— EmbSim(q, d)
    + 0.10 Ã— HostRank(d)
    + 0.06 Ã— AnchorMatch(d|q)
    + 0.05 Ã— StructuredBoost(d, q)
    + 0.04 Ã— FreshnessDecay(d)
    + 0.03 Ã— URLQuality(d)
    - 0.08 Ã— Spamness(d)
    + 0.04 Ã— IntentAlign(q, d)
  
  Total weight: 1.02 (slightly above 1.0 to allow for high-quality docs to stand out)
  
  ### Feature Weights Rationale
  - **BM25 (0.55)**: Dominant signal, lexical match is fundamental
  - **EmbSim (0.15)**: Semantic understanding, disambiguates intent
  - **HostRank (0.10)**: Authority/trust, breaks ties for similar content
  - **AnchorMatch (0.06)**: External validation, how others describe the page
  - **StructuredBoost (0.05)**: Reward rich structured data
  - **FreshnessDecay (0.04)**: Slight recency preference
  - **URLQuality (0.03)**: Small boost for clean, readable URLs
  - **Spamness (-0.08)**: Strong penalty for spam (negative weight)
  - **IntentAlign (0.04)**: Reward intent match (navigational/transactional)
  
  ### Tuning Method
  - Initial: Manual weights based on literature (BM25 dominant)
  - Optimization: Grid search on proxy metrics (NDCG@10, nav success)
  - Online learning: Pairwise LTR from click data (nightly updates)
  - Validation: A/B testing via interleaving
  
  ### Example Calculation
  Query: "best machine learning books"
  Document: High-quality Amazon book listing
  
  Feature Values:
  - BM25 = 22.5 (strong keyword match)
  - EmbSim = 0.82 (high semantic similarity)
  - HostRank = 0.88 (amazon.com has high authority)
  - AnchorMatch = 0.35 (some inlinks with "ML books")
  - StructuredBoost = 1.3 (Book schema.org)
  - FreshnessDecay = 0.95 (recent publication)
  - URLQuality = 0.85 (clean Amazon URL)
  - Spamness = 0.02 (very low spam probability)
  - IntentAlign = 0.85 (transactional intent match)
  
  FinalScore Calculation:
    = 0.55 Ã— 22.5 + 0.15 Ã— 0.82 + 0.10 Ã— 0.88 + 0.06 Ã— 0.35 
    + 0.05 Ã— 1.3 + 0.04 Ã— 0.95 + 0.03 Ã— 0.85 - 0.08 Ã— 0.02 + 0.04 Ã— 0.85
    = 12.375 + 0.123 + 0.088 + 0.021 + 0.065 + 0.038 + 0.0255 - 0.0016 + 0.034
    = 12.768
  
  High score â†’ Ranks near top of results
  
  ### Tuning History
  Version | Date | Changes | Impact
  v1.0 | 2024-01 | Initial weights from literature | Baseline
  v1.1 | 2024-02 | Increased EmbSim 0.10â†’0.15 | +12% NDCG@10
  v1.2 | 2024-03 | Added IntentAlign feature | +8% Nav success
  v1.3 | 2024-04 | Increased Spamness penalty 0.05â†’0.08 | -40% spam in top-10
  v2.0 | 2024-05 | Pairwise LTR from click data | +15% CTR@1
  ```

### Special Features & Signals

- [ ] **Query Expansion & Synonym Handling**
  - Lexicon-based expansion
  - Embedding-based related terms
  - Cross-language expansion
  - Threshold and limits (â‰¤3 expansions)
  - Impact on BM25 score

- [ ] **Spell Correction Models**
  - Edit distance candidates
  - Corpus frequency validation
  - Embedding-based semantic validation
  - Auto-correction vs. suggestion logic
  - Confidence thresholds

- [ ] **Stopword Filtering (Context-Aware)**
  - IDF-based automatic detection
  - Context rules (single-word queries, entity names, quoted phrases)
  - Multi-stage retrieval (filtered â†’ original fallback)
  - Impact on retrieval and ranking

- [ ] **Deduplication & Near-Duplicate Detection**
  - URL-level deduplication
  - Content simhash/shingles
  - Clustering threshold
  - Representative selection (highest FinalScore)

- [ ] **Diversification (MMR)**
  - Maximal Marginal Relevance formula
  - Lambda parameter (relevance vs. diversity trade-off)
  - Domain repetition limits (â‰¤3 results per domain in top-10)
  - Sub-intent coverage

### Language-Specific Processing

- [ ] **Universal Language Detection**
  - N-gram based detection (100+ languages)
  - Confidence scoring
  - Fallback handling
  - Script detection (Latin, Arabic, Cyrillic, CJK, etc.)

- [ ] **Script-Specific Normalization**
  - Arabic ZWNJ handling
  - CJK word segmentation
  - Cyrillic variants unification
  - Diacritic handling (optional removal)

- [ ] **Cross-Language Features**
  - Multilingual embeddings
  - Universal BM25 (no language config needed)
  - Language-agnostic spam detection
  - Universal quality signals

## Technical Approach

### Documentation Structure
```
docs/
â”œâ”€â”€ features/
â”‚   â”œâ”€â”€ README.md                    # Feature glossary overview
â”‚   â”œâ”€â”€ lexical/
â”‚   â”‚   â”œâ”€â”€ bm25.md                  # BM25 detailed documentation
â”‚   â”‚   â”œâ”€â”€ anchor-match.md
â”‚   â”‚   â””â”€â”€ url-quality.md
â”‚   â”œâ”€â”€ semantic/
â”‚   â”‚   â”œâ”€â”€ embeddings.md            # Embedding similarity
â”‚   â”‚   â”œâ”€â”€ spell-correction.md
â”‚   â”‚   â””â”€â”€ query-expansion.md
â”‚   â”œâ”€â”€ authority/
â”‚   â”‚   â”œâ”€â”€ hostrank.md              # PageRank/authority
â”‚   â”‚   â””â”€â”€ structured-boost.md
â”‚   â”œâ”€â”€ quality/
â”‚   â”‚   â”œâ”€â”€ spam-detection.md
â”‚   â”‚   â””â”€â”€ freshness.md
â”‚   â”œâ”€â”€ intent/
â”‚   â”‚   â””â”€â”€ intent-classification.md
â”‚   â””â”€â”€ final-score.md               # FinalScore formula
```

### Feature Visualization
```python
# Example: Feature importance visualization
import matplotlib.pyplot as plt
import numpy as np

features = ['BM25', 'EmbSim', 'HostRank', 'AnchorMatch', 
            'StructuredBoost', 'Freshness', 'URLQuality', 
            'Spamness (penalty)', 'IntentAlign']
weights = [0.55, 0.15, 0.10, 0.06, 0.05, 0.04, 0.03, -0.08, 0.04]

plt.figure(figsize=(10, 6))
colors = ['green' if w > 0 else 'red' for w in weights]
plt.barh(features, weights, color=colors)
plt.xlabel('Feature Weight in FinalScore')
plt.title('Ranking Feature Importance')
plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)
plt.tight_layout()
plt.savefig('docs/features/feature-weights.png')
```

### Interactive Feature Calculator
```html
<!-- Feature Calculator Tool -->
<!DOCTYPE html>
<html>
<head>
    <title>Ranking Score Calculator</title>
</head>
<body>
    <h1>FinalScore Calculator</h1>
    <form id="scoreForm">
        <label>BM25 Score: <input type="number" step="0.01" id="bm25" value="20"></label><br>
        <label>EmbSim (0-1): <input type="number" step="0.01" min="0" max="1" id="embsim" value="0.8"></label><br>
        <label>HostRank (0-1): <input type="number" step="0.01" min="0" max="1" id="hostrank" value="0.7"></label><br>
        <!-- More inputs... -->
        <button type="button" onclick="calculateScore()">Calculate</button>
    </form>
    <h2>Final Score: <span id="result">0.00</span></h2>
    
    <script>
    function calculateScore() {
        const bm25 = parseFloat(document.getElementById('bm25').value);
        const embsim = parseFloat(document.getElementById('embsim').value);
        const hostrank = parseFloat(document.getElementById('hostrank').value);
        // ... get other values
        
        const finalScore = 
            0.55 * bm25 +
            0.15 * embsim +
            0.10 * hostrank +
            // ... rest of formula
        
        document.getElementById('result').textContent = finalScore.toFixed(2);
    }
    </script>
</body>
</html>
```

## Acceptance Criteria

### Documentation Completeness
- [ ] All 9 ranking features fully documented with formulas
- [ ] FinalScore formula explained with weight rationale
- [ ] Each feature has 2+ practical examples
- [ ] Range and typical values documented for each feature
- [ ] Language handling explained for universal features
- [ ] Implementation locations referenced (file paths)

### Mathematical Accuracy
- [ ] All formulas mathematically correct and validated
- [ ] Example calculations verified (no errors)
- [ ] Parameter values match implementation
- [ ] Range normalization documented
- [ ] Edge cases explained (division by zero, etc.)

### Practical Usability
- [ ] Non-technical explanations provided
- [ ] Visual aids (charts, graphs, diagrams)
- [ ] Comparison tables (feature importance)
- [ ] Troubleshooting guide (unexpected scores)
- [ ] Historical tuning context documented

### Code Integration
- [ ] Code snippets for feature extraction
- [ ] Links to implementation files
- [ ] Unit test examples
- [ ] Debug mode output examples
- [ ] Feature trace format documented

## Testing & Validation

### Formula Verification
```python
# Verify FinalScore implementation matches documentation
def test_final_score_calculation():
    features = {
        'bm25': 22.5,
        'embsim': 0.82,
        'hostrank': 0.88,
        'anchor_match': 0.35,
        'structured_boost': 1.3,
        'freshness_decay': 0.95,
        'url_quality': 0.85,
        'spamness': 0.02,
        'intent_align': 0.85
    }
    
    expected = (
        0.55 * features['bm25'] +
        0.15 * features['embsim'] +
        0.10 * features['hostrank'] +
        0.06 * features['anchor_match'] +
        0.05 * features['structured_boost'] +
        0.04 * features['freshness_decay'] +
        0.03 * features['url_quality'] -
        0.08 * features['spamness'] +
        0.04 * features['intent_align']
    )
    
    actual = calculate_final_score(features)
    
    assert abs(expected - actual) < 0.01, f"Expected {expected}, got {actual}"
```

### Example Validation
- [ ] Run documented examples through actual system
- [ ] Verify feature values match expectations
- [ ] Check FinalScore calculations
- [ ] Validate range claims
- [ ] Test edge cases

## Documentation

### Feature Glossary Index
Create `docs/features/README.md`:
```markdown
# Ranking Feature Glossary

Complete documentation of all ranking signals used in the universal multilingual search engine.

## Overview
The ranking system uses 9 primary features combined in a weighted formula to produce the final document score.

## Quick Reference

| Feature | Weight | Range | Purpose |
|---------|--------|-------|---------|
| BM25 | 0.55 | 0-100+ | Lexical relevance |
| EmbSim | 0.15 | 0-1 | Semantic similarity |
| HostRank | 0.10 | 0-1 | Domain authority |
| AnchorMatch | 0.06 | 0-1 | Anchor text relevance |
| StructuredBoost | 0.05 | 1.0-1.5 | Schema.org bonus |
| FreshnessDecay | 0.04 | 0-1 | Recency factor |
| URLQuality | 0.03 | 0-1 | URL pattern quality |
| Spamness | -0.08 | 0-1 | Spam penalty |
| IntentAlign | 0.04 | 0-1 | Intent match |

## Detailed Documentation
- [BM25 (Lexical Relevance)](lexical/bm25.md)
- [Embedding Similarity (Semantic)](semantic/embeddings.md)
- [HostRank (Authority)](authority/hostrank.md)
- [Final Score Formula](final-score.md)
- ... (all features)

## Use Cases
- **Debugging:** Use debug mode to see feature values
- **Optimization:** Understand feature importance for tuning
- **Training:** Learn ranking logic and behavior
- **Stakeholders:** Explain search quality factors
```

## Notes

### Best Practices
1. **Formulas First:** Start with mathematical definitions
2. **Examples Everywhere:** Show concrete calculations
3. **Visual Aids:** Use charts for feature importance
4. **Historical Context:** Document tuning history
5. **Code Links:** Reference implementation files

### Common Pitfalls
- âŒ Vague descriptions without formulas
- âŒ Examples that don't match reality
- âŒ Missing range and typical value information
- âŒ No explanation of feature interactions
- âŒ Outdated documentation after tuning changes

### Tools & Resources
- [Information Retrieval Textbook](https://nlp.stanford.edu/IR-book/)
- [BM25 Paper](https://en.wikipedia.org/wiki/Okapi_BM25)
- [PageRank Original Paper](http://ilpubs.stanford.edu:8090/422/)
- [Learning to Rank](https://en.wikipedia.org/wiki/Learning_to_rank)

## Success Metrics
- Feature glossary referenced in 90%+ of ranking discussions
- Zero incidents from feature misunderstanding
- New team members can explain FinalScore within 2 days
- Positive feedback on documentation clarity (survey NPS >8)
- Feature documentation kept up-to-date (reviewed quarterly)

## Related Tasks
- 15.2 - API Specification (debug mode references features)
- 15.4 - Troubleshooting Runbooks (uses feature knowledge)
- 08.3 - Parameter Optimization (tunes feature weights)
- 09.7 - Feature Gathering (implements feature extraction)

---

**Celebration Criteria:** ðŸŽ‰
- All 9 features fully documented with formulas and examples
- FinalScore formula explained and validated
- Interactive calculator deployed
- Positive feedback from team on documentation quality and usefulness

