# ðŸŽ¯ Task 05.5: Document Embedding Precomputation

## ðŸ“… Sprint Info
- **Duration:** 2 days
- **Milestone:** M3 - Semantic Processing
- **Priority:** P1
- **Depends On:** Task 05.4 (Embedding Inference Service) âœ…
- **Blocks:** Task 08.2 (Feature Fusion)
- **Assignee:** TBD

## ðŸŽ¬ What You'll Build
Batch precompute embeddings for all indexed documents and store them in the feature store. Enables fast semantic similarity computation during query-time re-ranking without real-time embedding inference overhead.

## ðŸ“‹ Daily Breakdown

### Day 1: Batch Processing Pipeline
- [ ] Design batch processing architecture
- [ ] Implement document text extraction from MongoDB
- [ ] Build batch embedding computation (chunk by 1000s)
- [ ] Add progress tracking and resumption
- [ ] Implement error handling and retry logic
- [ ] Store embeddings in Redis/MongoDB feature store
- [ ] Add embedding versioning and metadata
- [ ] Performance testing: target 1000+ docs/sec

### Day 2: Storage Optimization & Incremental Updates
- [ ] Implement vector quantization (optional, for space saving)
- [ ] Build incremental update system for new documents
- [ ] Add embedding staleness detection
- [ ] Create embedding cleanup for deleted documents
- [ ] Build monitoring and alerting
- [ ] Performance optimization and benchmarking
- [ ] Documentation and operational runbook

## âœ… Acceptance Criteria
- [ ] All indexed documents have precomputed embeddings
- [ ] Batch processing: 1000+ documents/sec throughput
- [ ] Embeddings stored with version tags and timestamps
- [ ] Incremental updates work for new/modified documents
- [ ] Storage efficient: <500MB per 1M documents (with quantization)
- [ ] Embedding retrieval latency: <1ms per document
- [ ] Staleness detection: embeddings refreshed within 24 hours
- [ ] Error rate: <0.1% failed embeddings with retry

## ðŸ§ª Testing Checklist
- [ ] Unit tests for batch processing (10+ test cases)
- [ ] Large-scale integration test (100K+ documents)
- [ ] Incremental update tests
- [ ] Error recovery tests (network failures, API errors)
- [ ] Performance benchmarks (throughput, latency)
- [ ] Storage efficiency tests (compression, quantization)
- [ ] Multi-language document tests (10+ languages)

## ðŸŽ‰ Celebration Criteria (Definition of Done)
âœ… **Demo Ready:** Show embedding lookup for 1M documents with <1ms latency
âœ… **Metric Met:** Batch processing completes 1M docs in <15 minutes
âœ… **Integration:** Embeddings successfully used in re-ranking pipeline
âœ… **Reliability:** 7-day continuous operation without failures

**ðŸŽŠ Celebration Moment:** Semantic re-ranking is now blazing fast!

## ðŸ“¦ Deliverables
- `src/python/embeddings/batch_precompute.py` (300-400 lines)
- `src/python/embeddings/incremental_updater.py` (200 lines)
- `src/python/embeddings/storage_manager.py` (150 lines)
- `tests/test_batch_precompute.py` (40+ test cases)
- `scripts/precompute_embeddings.sh` (batch job script)
- `docs/operations/embedding-precomputation.md`
- `monitoring/embedding_alerts.yaml`

## ðŸ”— Dependencies & Integration

### Input
```python
# Documents from MongoDB
{
    "doc_id": str,
    "title": str,
    "body": str,
    "language": str,
    "indexed_at": datetime,
    "modified_at": datetime
}
```

### Output
```python
# Precomputed embeddings in Redis
{
    "doc_id": str,
    "embedding": List[float],  # 300-dim vector
    "version": str,  # e.g., "fasttext_v1"
    "computed_at": datetime,
    "dimension": int
}

# Feature Store Schema (Redis)
Key: "doc_emb:v1:{doc_id}"
Value: JSON({
    "emb": [0.123, -0.456, ...],
    "dim": 300,
    "ver": "v1",
    "ts": 1699564800
})
TTL: 30 days (refresh if stale)
```

### External Dependencies
- MongoDB client (document retrieval)
- Redis client (embedding storage)
- Embedding inference service (from Task 05.4)
- NumPy (vector operations)
- Python multiprocessing (parallel batch processing)

## ðŸš€ Next Steps
âž¡ï¸ **Task 08.2: Feature Fusion Pipeline** (4 days)
- Uses precomputed embeddings for semantic scoring
- Combines with BM25, authority, quality signals

âž¡ï¸ **Task 05.7: Nightly Embedding Refresh** (2 days)
- Automated job to refresh stale embeddings
- Handles new documents and updates

## ðŸ’¡ Tips & Resources

### Common Pitfalls
- âš ï¸ **Memory overflow**: Process documents in batches, don't load all at once
- âš ï¸ **API rate limiting**: Batch embedding requests to inference service
- âš ï¸ **Stale embeddings**: Track modification timestamps for refresh
- âš ï¸ **Storage bloat**: Use TTL and cleanup for deleted documents
- âš ï¸ **Concurrency issues**: Use locks for incremental updates

### Helpful Resources
- [Redis Mass Insertion](https://redis.io/docs/manual/pipelining/)
- [Python Multiprocessing Best Practices](https://docs.python.org/3/library/multiprocessing.html)
- [Vector Quantization Techniques](https://en.wikipedia.org/wiki/Vector_quantization)
- [MongoDB Batch Operations](https://www.mongodb.com/docs/manual/core/bulk-write-operations/)

### Example Code

#### Batch Precomputation
```python
import redis
import pymongo
from typing import List, Dict
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed

class DocumentEmbeddingPrecomputer:
    def __init__(self, mongo_uri, redis_url, embedding_service_url):
        self.mongo_client = pymongo.MongoClient(mongo_uri)
        self.redis_client = redis.Redis.from_url(redis_url)
        self.embedding_service = EmbeddingServiceClient(embedding_service_url)
        self.db = self.mongo_client['search-engine']
        self.collection = self.db['documents']
        
    def precompute_all(self, batch_size=1000, max_workers=8):
        """Precompute embeddings for all documents"""
        total_docs = self.collection.count_documents({})
        processed = 0
        
        cursor = self.collection.find({}, {"doc_id": 1, "title": 1, "body": 1, "language": 1})
        
        batch = []
        for doc in cursor:
            batch.append(doc)
            
            if len(batch) >= batch_size:
                self._process_batch(batch, max_workers)
                processed += len(batch)
                logging.info(f"Processed {processed}/{total_docs} documents ({processed/total_docs*100:.1f}%)")
                batch = []
        
        # Process remaining documents
        if batch:
            self._process_batch(batch, max_workers)
            processed += len(batch)
        
        logging.info(f"Precomputation complete: {processed} documents")
    
    def _process_batch(self, batch: List[Dict], max_workers: int):
        """Process a batch of documents in parallel"""
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(self._compute_and_store, doc): doc 
                for doc in batch
            }
            
            for future in as_completed(futures):
                doc = futures[future]
                try:
                    future.result()
                except Exception as e:
                    logging.error(f"Error processing doc {doc['doc_id']}: {e}")
    
    def _compute_and_store(self, doc: Dict):
        """Compute embedding and store in Redis"""
        # Combine title and body
        text = f"{doc.get('title', '')} {doc.get('body', '')}"[:5000]
        
        # Get embedding from service
        embedding = self.embedding_service.get_embedding(text, doc.get('language', 'en'))
        
        # Store in Redis
        key = f"doc_emb:v1:{doc['doc_id']}"
        value = {
            "emb": embedding,
            "dim": len(embedding),
            "ver": "v1",
            "ts": int(time.time())
        }
        
        # Store with 30-day TTL
        self.redis_client.setex(key, 30 * 24 * 3600, json.dumps(value))
        
        logging.debug(f"Stored embedding for doc {doc['doc_id']}")

# Incremental updates
class IncrementalEmbeddingUpdater:
    def __init__(self, mongo_uri, redis_url, embedding_service_url):
        self.precomputer = DocumentEmbeddingPrecomputer(
            mongo_uri, redis_url, embedding_service_url
        )
        self.redis_client = redis.Redis.from_url(redis_url)
        
    def update_new_documents(self):
        """Update embeddings for newly added documents"""
        # Find documents without embeddings
        cursor = self.precomputer.collection.find({})
        
        docs_to_update = []
        for doc in cursor:
            key = f"doc_emb:v1:{doc['doc_id']}"
            if not self.redis_client.exists(key):
                docs_to_update.append(doc)
        
        if docs_to_update:
            logging.info(f"Updating {len(docs_to_update)} new documents")
            self.precomputer._process_batch(docs_to_update, max_workers=8)
        
    def refresh_stale_embeddings(self, staleness_hours=24):
        """Refresh embeddings older than staleness threshold"""
        current_time = int(time.time())
        staleness_seconds = staleness_hours * 3600
        
        # Scan Redis for stale embeddings
        stale_docs = []
        for key in self.redis_client.scan_iter("doc_emb:v1:*"):
            value = json.loads(self.redis_client.get(key))
            if current_time - value['ts'] > staleness_seconds:
                doc_id = key.decode().split(":")[-1]
                stale_docs.append(doc_id)
        
        if stale_docs:
            logging.info(f"Refreshing {len(stale_docs)} stale embeddings")
            docs = list(self.precomputer.collection.find({"doc_id": {"$in": stale_docs}}))
            self.precomputer._process_batch(docs, max_workers=8)
```

#### Usage Script
```bash
#!/bin/bash
# scripts/precompute_embeddings.sh

python3 -m src.python.embeddings.batch_precompute \
    --mongo-uri "mongodb://admin:password123@mongodb:27017" \
    --redis-url "redis://redis:6379/0" \
    --embedding-service "http://embedding-service:8080" \
    --batch-size 1000 \
    --max-workers 16 \
    --log-level INFO
```

## ðŸ“Š Success Metrics
- **Throughput:** 1000+ documents/sec
- **Coverage:** 100% of indexed documents
- **Latency:** <1ms per embedding retrieval
- **Storage Efficiency:** <500MB per 1M documents
- **Staleness:** <1% embeddings older than 24 hours
- **Error Rate:** <0.1% failed computations

## ðŸŽ“ Learning Outcomes
After completing this task, you will:
- âœ… Design efficient batch processing pipelines
- âœ… Implement incremental data updates
- âœ… Optimize storage for large-scale vector data
- âœ… Handle distributed system failures gracefully
- âœ… Monitor and maintain data freshness

---

**Pre-compute once, serve millions! ðŸš€ðŸ’¾**

