# ðŸŽ¯ Task 05.7: Nightly Lexicon Export & Refresh

## ðŸ“… Sprint Info
- **Duration:** 2 days
- **Milestone:** M3 - Semantic Processing  
- **Priority:** P1
- **Depends On:** Task 05.6 (Spell Correction Models) âœ…
- **Blocks:** None (continuous operation)
- **Assignee:** TBD

## ðŸŽ¬ What You'll Build
Automated nightly batch job that refreshes semantic lexicons, spell correction vocabularies, and query expansion terms from the updated corpus. Keeps NLP models fresh and accurate as new content is indexed.

## ðŸ“‹ Daily Breakdown

### Day 1: Batch Job Architecture
- [ ] Design nightly job orchestration (cron/Airflow/k8s CronJob)
- [ ] Implement incremental corpus analysis
- [ ] Build synonym/related term mining pipeline
- [ ] Update vocabulary frequencies from new documents
- [ ] Refresh character n-gram statistics
- [ ] Export lexicons to Redis with versioning
- [ ] Add job monitoring and alerting
- [ ] Create failure recovery mechanisms

### Day 2: Optimization & Deployment
- [ ] Optimize for large-scale processing (100M+ documents)
- [ ] Implement checkpointing for long-running jobs
- [ ] Add data validation and quality checks
- [ ] Build rollback mechanism for bad updates
- [ ] Create monitoring dashboard
- [ ] Write operational runbook
- [ ] Deploy to production scheduler
- [ ] Test full end-to-end pipeline

## âœ… Acceptance Criteria
- [ ] Nightly job completes within 1 hour SLA for 100M documents
- [ ] Lexicon updates deploy atomically (no downtime)
- [ ] Version control: old lexicons retained for 7 days
- [ ] Incremental processing: only analyze new/modified documents
- [ ] Automatic failure alerts and retry logic
- [ ] Data validation: quality checks before deployment
- [ ] Rollback capability: revert to previous version in <5 minutes
- [ ] Monitoring: job duration, success rate, lexicon size metrics

## ðŸ§ª Testing Checklist
- [ ] Unit tests for lexicon export (15+ test cases)
- [ ] Integration tests with full pipeline
- [ ] Large-scale performance tests (1M+ documents)
- [ ] Failure recovery tests (network, OOM, timeout)
- [ ] Version management tests
- [ ] Rollback tests
- [ ] Data validation tests
- [ ] End-to-end monitoring tests

## ðŸŽ‰ Celebration Criteria (Definition of Done)
âœ… **Demo Ready:** Show lexicon updates propagating to live queries
âœ… **Metric Met:** Job completes in <1 hour with 99.9% success rate
âœ… **Integration:** Seamless updates with zero query serving disruption
âœ… **Reliability:** 30-day continuous operation without manual intervention

**ðŸŽŠ Celebration Moment:** Set it and forget it - automated NLP freshness!

## ðŸ“¦ Deliverables
- `src/python/batch/nightly_lexicon_refresh.py` (400-500 lines)
- `src/python/batch/incremental_analyzer.py` (250 lines)
- `src/python/batch/lexicon_exporter.py` (200 lines)
- `src/python/batch/version_manager.py` (150 lines)
- `tests/test_nightly_refresh.py` (40+ test cases)
- `k8s/cronjobs/lexicon-refresh.yaml` (k8s CronJob)
- `monitoring/lexicon_refresh_dashboard.json` (Grafana dashboard)
- `runbooks/lexicon-refresh-operations.md`

## ðŸ”— Dependencies & Integration

### Job Schedule
```yaml
# Kubernetes CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: lexicon-refresh
spec:
  schedule: "0 2 * * *"  # 2 AM daily
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: lexicon-refresh
            image: search-engine/lexicon-refresh:latest
            env:
            - name: MONGODB_URI
              value: "mongodb://mongodb:27017"
            - name: REDIS_URL
              value: "redis://redis:6379"
            resources:
              limits:
                memory: "8Gi"
                cpu: "4"
          restartPolicy: OnFailure
```

### Lexicon Outputs
```python
# Synonym lexicon
{
    "word": "search",
    "related_terms": [
        {"term": "find", "score": 0.85},
        {"term": "lookup", "score": 0.78},
        {"term": "query", "score": 0.75}
    ],
    "version": "v1.20231115",
    "last_updated": "2023-11-15T02:15:30Z"
}

# Spell correction vocabulary
{
    "word": "search",
    "frequency": 125000,
    "doc_frequency": 85000,
    "variants": ["searhc", "searh", "serch"],
    "version": "v1.20231115"
}

# Query expansion lexicon
{
    "term": "machine learning",
    "expansions": [
        {"term": "ML", "score": 0.92},
        {"term": "artificial intelligence", "score": 0.75},
        {"term": "deep learning", "score": 0.70}
    ],
    "max_expansions": 3
}
```

### External Dependencies
- MongoDB (corpus data)
- Redis (lexicon storage)
- Kubernetes or cron (job scheduling)
- Prometheus/Grafana (monitoring)
- Airflow (optional, for complex orchestration)

## ðŸš€ Next Steps
âž¡ï¸ **Continuous Operation:** Job runs nightly automatically
âž¡ï¸ **Monitoring:** Track job health and lexicon quality metrics
âž¡ï¸ **Optimization:** Tune processing for new data patterns

## ðŸ’¡ Tips & Resources

### Common Pitfalls
- âš ï¸ **Memory overflow**: Stream processing for large corpus, don't load all
- âš ï¸ **Race conditions**: Use atomic Redis updates with versioning
- âš ï¸ **Stale cache**: Invalidate query caches after lexicon updates
- âš ï¸ **Long jobs**: Add checkpointing every N documents
- âš ï¸ **Bad data**: Validate lexicons before deploying to production

### Helpful Resources
- [Kubernetes CronJobs](https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/)
- [Redis Atomic Operations](https://redis.io/docs/manual/transactions/)
- [Apache Airflow Tutorial](https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html)
- [Batch Processing Best Practices](https://cloud.google.com/architecture/batch-processing-best-practices)

### Example Code

#### Nightly Refresh Job
```python
import logging
import time
from datetime import datetime
import redis
import pymongo
from typing import Dict, List

class NightlyLexiconRefresh:
    def __init__(self, mongo_uri, redis_url):
        self.mongo_client = pymongo.MongoClient(mongo_uri)
        self.redis_client = redis.Redis.from_url(redis_url)
        self.db = self.mongo_client['search-engine']
        self.version = f"v1.{datetime.now().strftime('%Y%m%d')}"
        
    def run(self):
        """Execute nightly refresh job"""
        start_time = time.time()
        logging.info(f"Starting lexicon refresh {self.version}")
        
        try:
            # Step 1: Identify new/modified documents since last run
            new_docs = self.get_incremental_documents()
            logging.info(f"Processing {len(new_docs)} new/modified documents")
            
            # Step 2: Update vocabularies
            self.update_vocabularies(new_docs)
            
            # Step 3: Mine synonyms and related terms
            self.mine_semantic_relations(new_docs)
            
            # Step 4: Update spell correction models
            self.update_spell_correction(new_docs)
            
            # Step 5: Export to Redis with new version
            self.export_lexicons()
            
            # Step 6: Validate new lexicons
            self.validate_lexicons()
            
            # Step 7: Atomic swap to new version
            self.activate_new_version()
            
            # Step 8: Cleanup old versions (keep 7 days)
            self.cleanup_old_versions()
            
            duration = time.time() - start_time
            logging.info(f"Lexicon refresh complete in {duration:.1f}s")
            
            # Record metrics
            self.record_metrics(duration, len(new_docs), success=True)
            
        except Exception as e:
            logging.error(f"Lexicon refresh failed: {e}")
            self.record_metrics(time.time() - start_time, 0, success=False)
            raise
    
    def get_incremental_documents(self) -> List[Dict]:
        """Get documents added/modified since last run"""
        last_run = self.redis_client.get("lexicon:last_refresh")
        
        if last_run:
            last_run_time = datetime.fromisoformat(last_run.decode())
            query = {"modified_at": {"$gt": last_run_time}}
        else:
            # First run: process all documents
            query = {}
        
        docs = list(self.db.documents.find(query).limit(10000000))
        
        # Update last run timestamp
        self.redis_client.set("lexicon:last_refresh", datetime.now().isoformat())
        
        return docs
    
    def update_vocabularies(self, docs: List[Dict]):
        """Update word frequencies"""
        word_counts = Counter()
        
        for doc in docs:
            text = f"{doc.get('title', '')} {doc.get('body', '')}"
            words = self.tokenize(text)
            word_counts.update(words)
        
        # Merge with existing vocabulary
        for word, count in word_counts.items():
            key = f"vocab:v1:{word}"
            self.redis_client.hincrby(key, "frequency", count)
            self.redis_client.hincrby(key, "doc_frequency", 1)
            self.redis_client.hset(key, "version", self.version)
    
    def mine_semantic_relations(self, docs: List[Dict]):
        """Mine synonyms and related terms"""
        # Use co-occurrence or embedding similarity
        # ... (implementation from Task 05.1, 05.2)
        pass
    
    def export_lexicons(self):
        """Export lexicons to versioned Redis keys"""
        # Export with new version
        # ... (implementation details)
        pass
    
    def activate_new_version(self):
        """Atomically switch to new lexicon version"""
        # Use Redis MULTI/EXEC for atomic update
        pipeline = self.redis_client.pipeline()
        pipeline.set("lexicon:current_version", self.version)
        pipeline.set(f"lexicon:version:{self.version}:activated_at", 
                    datetime.now().isoformat())
        pipeline.execute()
        
        logging.info(f"Activated lexicon version {self.version}")
    
    def cleanup_old_versions(self, retention_days=7):
        """Remove lexicon versions older than retention period"""
        cutoff_date = datetime.now() - timedelta(days=retention_days)
        
        for key in self.redis_client.scan_iter("lexicon:version:*"):
            version_date_str = key.decode().split(":")[2].replace("v1.", "")
            version_date = datetime.strptime(version_date_str, "%Y%m%d")
            
            if version_date < cutoff_date:
                # Delete old version
                self.redis_client.delete(key)
                logging.info(f"Cleaned up old version: {key.decode()}")
    
    def record_metrics(self, duration: float, docs_processed: int, success: bool):
        """Record job metrics for monitoring"""
        metrics = {
            "job_duration_seconds": duration,
            "documents_processed": docs_processed,
            "success": 1 if success else 0,
            "timestamp": datetime.now().isoformat(),
            "version": self.version
        }
        
        # Store in Redis for Prometheus/Grafana
        self.redis_client.lpush("metrics:lexicon_refresh", json.dumps(metrics))
        self.redis_client.ltrim("metrics:lexicon_refresh", 0, 999)  # Keep 1000 entries

# Run job
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    job = NightlyLexiconRefresh(
        mongo_uri=os.getenv("MONGODB_URI"),
        redis_url=os.getenv("REDIS_URL")
    )
    
    job.run()
```

## ðŸ“Š Success Metrics
- **Duration:** Job completes in <1 hour (SLA)
- **Success Rate:** â‰¥99.9% successful runs
- **Freshness:** Lexicons updated within 24 hours of new content
- **Downtime:** Zero query serving disruption during updates
- **Data Quality:** â‰¥95% validation pass rate
- **Recovery:** Automatic retry completes within 2 hours

## ðŸŽ“ Learning Outcomes
After completing this task, you will:
- âœ… Design robust batch processing pipelines
- âœ… Implement versioning and atomic deployments
- âœ… Build failure recovery and monitoring
- âœ… Handle large-scale incremental data processing
- âœ… Operate production ML data pipelines

---

**Fresh lexicons every night, automatically! ðŸŒ™ðŸ”„**

