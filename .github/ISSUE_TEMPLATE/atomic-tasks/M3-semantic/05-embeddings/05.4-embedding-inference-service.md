# üéØ Task 05.4: Embedding Inference Service

## üìÖ Sprint Info
- **Duration:** 3 days
- **Milestone:** M3 - Semantic Processing
- **Priority:** P1
- **Depends On:** Task 05.3 (Subword Embeddings) ‚úÖ
- **Blocks:** Task 06.2 (Re-ranking Pipeline)
- **Assignee:** TBD

## üé¨ What You'll Build
Build a high-performance embedding inference service (HTTP/gRPC) that serves query and document embeddings with intelligent caching. Powers semantic re-ranking and query expansion for any language.

## üìã Daily Breakdown

### Day 1: Service Architecture & API Design
- [ ] Design REST API endpoints for embedding service
- [ ] Implement gRPC service definition (optional)
- [ ] Load trained embeddings (PPMI/SVD + FastText)
- [ ] Build embedding lookup with fallback (FastText ‚Üí SVD ‚Üí zero)
- [ ] Add batch processing support
- [ ] Create health check and metrics endpoints
- [ ] Document API specification (OpenAPI/Swagger)

### Day 2: Caching & Performance Optimization
- [ ] Implement Redis caching layer for hot queries
- [ ] Add LRU memory cache for frequently accessed embeddings
- [ ] Build batch processing for multiple queries
- [ ] Optimize serialization (Protocol Buffers or MessagePack)
- [ ] Add request deduplication
- [ ] Implement connection pooling
- [ ] Performance testing: target <10ms P95 latency

### Day 3: Integration, Testing & Deployment
- [ ] Create C++ gRPC client for integration
- [ ] Build Python client library with examples
- [ ] Add comprehensive unit and integration tests
- [ ] Load testing (wrk, vegeta, or locust)
- [ ] Docker containerization
- [ ] Kubernetes deployment manifests
- [ ] Monitoring and alerting setup
- [ ] Documentation and runbook

## ‚úÖ Acceptance Criteria
- [ ] REST/gRPC API successfully serves embeddings for any language
- [ ] P95 latency ‚â§10ms for single query embedding
- [ ] Batch processing: 100+ embeddings in <50ms
- [ ] Cache hit rate ‚â•70% for head queries
- [ ] Graceful handling of OOV words (via FastText)
- [ ] Service uptime ‚â•99.9% with health checks
- [ ] Memory usage: <4GB for 1M vocabulary
- [ ] Throughput: 1000+ requests/sec sustained

## üß™ Testing Checklist
- [ ] Unit tests for embedding lookup (20+ test cases)
- [ ] Cache hit/miss behavior tests
- [ ] OOV word handling tests
- [ ] Batch processing tests
- [ ] Load tests (sustained 1000 QPS)
- [ ] Failover and error handling tests
- [ ] Multi-language embedding tests (10+ languages)
- [ ] Performance regression tests

## üéâ Celebration Criteria (Definition of Done)
‚úÖ **Demo Ready:** Query embedding service from C++ and Python clients
‚úÖ **Metric Met:** P95 latency <10ms with 1000 QPS sustained
‚úÖ **Integration:** Successfully integrated into re-ranking pipeline
‚úÖ **Reliability:** 24-hour load test passes with no crashes

**üéä Celebration Moment:** Real-time semantic search is now possible!

## üì¶ Deliverables
- `src/python/embedding_service/server.py` (250-300 lines)
- `src/python/embedding_service/cache.py` (150 lines)
- `include/embeddings/EmbeddingClient.h` (C++ client, 100 lines)
- `src/embeddings/EmbeddingClient.cpp` (C++ client, 200 lines)
- `tests/test_embedding_service.py` (50+ test cases)
- `tests/test_embedding_client.cpp` (30+ test cases)
- `docker/embedding-service/Dockerfile`
- `k8s/embedding-service.yaml`
- `docs/api/embedding-service-api.md`

## üîó Dependencies & Integration

### API Specification

#### REST API
```http
POST /api/v1/embeddings/query
Content-Type: application/json

{
  "text": "search query in any language",
  "language": "auto",  # Optional: auto-detect
  "model_version": "v1"
}

Response 200 OK:
{
  "embedding": [0.123, -0.456, ...],  # 300-dim vector
  "dimension": 300,
  "language": "en",
  "cache_hit": true,
  "latency_ms": 5.2
}
```

```http
POST /api/v1/embeddings/batch
Content-Type: application/json

{
  "texts": ["query 1", "query 2", ...],
  "language": "auto"
}

Response 200 OK:
{
  "embeddings": [[...], [...], ...],
  "dimension": 300,
  "count": 10
}
```

#### gRPC Service
```protobuf
service EmbeddingService {
  rpc GetQueryEmbedding(EmbeddingRequest) returns (EmbeddingResponse);
  rpc GetBatchEmbeddings(BatchEmbeddingRequest) returns (BatchEmbeddingResponse);
  rpc GetDocumentEmbedding(DocumentRequest) returns (EmbeddingResponse);
}

message EmbeddingRequest {
  string text = 1;
  string language = 2;  // Optional
  string model_version = 3;
}

message EmbeddingResponse {
  repeated float embedding = 1;
  int32 dimension = 2;
  string language = 3;
  bool cache_hit = 4;
  float latency_ms = 5;
}
```

### C++ Client Usage
```cpp
#include "embeddings/EmbeddingClient.h"

EmbeddingClient client("http://localhost:8080");

// Single query
auto embedding = client.get_query_embedding("search query");

// Batch queries
std::vector<std::string> queries = {"query1", "query2"};
auto embeddings = client.get_batch_embeddings(queries);

// Compute similarity
double sim = client.compute_similarity(emb1, emb2);
```

### External Dependencies
- FastAPI 0.95+ or Flask 2.0+ (HTTP server)
- gRPC 1.50+ (optional, for gRPC service)
- Redis-py 4.0+ (caching layer)
- NumPy 1.20+ (vector operations)
- httpx or requests (for C++ client HTTP calls)

## üöÄ Next Steps
‚û°Ô∏è **Task 06.2: Re-ranking Pipeline** (4 days)
- Uses embedding service for semantic similarity scoring
- Integrates into query processing pipeline

‚û°Ô∏è **Task 05.5: Document Embedding Precomputation** (2 days)
- Batch precompute embeddings for all documents
- Store in feature store for fast retrieval

## üí° Tips & Resources

### Common Pitfalls
- ‚ö†Ô∏è **Cold start**: Preload models and warm cache before serving traffic
- ‚ö†Ô∏è **Memory leaks**: Use proper cleanup for embedding lookups
- ‚ö†Ô∏è **Timeout issues**: Set appropriate timeouts for client requests
- ‚ö†Ô∏è **Serialization overhead**: Use binary formats (protobuf, msgpack)
- ‚ö†Ô∏è **Cache invalidation**: Handle model updates gracefully

### Helpful Resources
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [gRPC Python Tutorial](https://grpc.io/docs/languages/python/)
- [Redis Caching Strategies](https://redis.io/docs/manual/patterns/)
- [Protocol Buffers Guide](https://developers.google.com/protocol-buffers)

### Example Code

#### Python FastAPI Service
```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import numpy as np
import redis
import hashlib
import json

app = FastAPI()
redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)

# Load models at startup
fasttext_model = load_fasttext_model("models/fasttext_v1.bin")
svd_embeddings = load_svd_embeddings("data/embeddings/svd_v1.npz")

class EmbeddingRequest(BaseModel):
    text: str
    language: str = "auto"
    model_version: str = "v1"

class EmbeddingResponse(BaseModel):
    embedding: List[float]
    dimension: int
    language: str
    cache_hit: bool
    latency_ms: float

@app.post("/api/v1/embeddings/query", response_model=EmbeddingResponse)
async def get_query_embedding(request: EmbeddingRequest):
    start_time = time.time()
    
    # Check cache
    cache_key = f"emb:v1:{hashlib.md5(request.text.encode()).hexdigest()}"
    cached = redis_client.get(cache_key)
    
    if cached:
        embedding = json.loads(cached)
        return EmbeddingResponse(
            embedding=embedding,
            dimension=len(embedding),
            language=request.language,
            cache_hit=True,
            latency_ms=(time.time() - start_time) * 1000
        )
    
    # Compute embedding
    embedding = fasttext_model.wv.get_vector(request.text, norm=True).tolist()
    
    # Cache result (TTL: 1 hour)
    redis_client.setex(cache_key, 3600, json.dumps(embedding))
    
    return EmbeddingResponse(
        embedding=embedding,
        dimension=len(embedding),
        language=request.language,
        cache_hit=False,
        latency_ms=(time.time() - start_time) * 1000
    )
```

#### C++ Client
```cpp
class EmbeddingClient {
public:
    EmbeddingClient(const std::string& service_url) 
        : service_url_(service_url) {}
    
    std::vector<float> get_query_embedding(const std::string& query) {
        nlohmann::json request_body = {
            {"text", query},
            {"language", "auto"},
            {"model_version", "v1"}
        };
        
        auto response = http_client_.post(
            service_url_ + "/api/v1/embeddings/query",
            request_body.dump()
        );
        
        auto json_response = nlohmann::json::parse(response.body);
        return json_response["embedding"].get<std::vector<float>>();
    }
    
    double compute_similarity(const std::vector<float>& a, 
                             const std::vector<float>& b) {
        double dot = 0.0, norm_a = 0.0, norm_b = 0.0;
        for (size_t i = 0; i < a.size(); i++) {
            dot += a[i] * b[i];
            norm_a += a[i] * a[i];
            norm_b += b[i] * b[i];
        }
        return dot / (std::sqrt(norm_a) * std::sqrt(norm_b));
    }

private:
    std::string service_url_;
    HTTPClient http_client_;
};
```

## üìä Success Metrics
- **Latency:** P95 <10ms, P99 <20ms
- **Throughput:** 1000+ QPS sustained
- **Cache Hit Rate:** ‚â•70% on production traffic
- **Availability:** ‚â•99.9% uptime
- **Memory:** <4GB for service + models
- **Accuracy:** Embedding quality maintained from training

## üéì Learning Outcomes
After completing this task, you will:
- ‚úÖ Build production-grade ML inference services
- ‚úÖ Implement effective caching strategies
- ‚úÖ Optimize for low-latency serving
- ‚úÖ Design language-agnostic APIs
- ‚úÖ Deploy and monitor ML services in production

---

**Semantic search at the speed of light! ‚ö°Ô∏èüîç**

