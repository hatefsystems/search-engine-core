# ðŸŽ¯ Task 05.2: PPMI & SVD Embeddings Training

## ðŸ“… Sprint Info
- **Duration:** 4 days
- **Milestone:** M3 - Semantic Processing
- **Priority:** P1
- **Depends On:** Task 05.1 (Co-occurrence Matrix) âœ…
- **Blocks:** Task 05.3 (Subword Embeddings)
- **Assignee:** TBD

## ðŸŽ¬ What You'll Build
Train universal distributional semantic embeddings using PPMI (Positive Pointwise Mutual Information) and truncated SVD. Create low-dimensional dense vectors that capture semantic relationships across any language.

## ðŸ“‹ Daily Breakdown

### Day 1: PPMI Computation
- [ ] Load co-occurrence matrix from Task 05.1
- [ ] Implement PPMI transformation algorithm
- [ ] Add smoothing and negative sampling
- [ ] Handle sparse matrix operations efficiently
- [ ] Validate PPMI values (should be non-negative)
- [ ] Save PPMI matrix in compressed format

### Day 2: SVD Dimensionality Reduction
- [ ] Implement truncated SVD (scikit-learn or scipy)
- [ ] Experiment with dimensions: 100, 200, 300
- [ ] Optimize for memory efficiency (streaming SVD)
- [ ] Store row embeddings (word vectors)
- [ ] Store column embeddings (context vectors)
- [ ] Add checkpointing for long computations

### Day 3: Quality Validation & Tuning
- [ ] Intrinsic evaluation: nearest neighbors
- [ ] Compute cosine similarities for test pairs
- [ ] Validate cross-language semantic connections
- [ ] Tune hyperparameters (smoothing, dimensions)
- [ ] Compare against baseline (random embeddings)
- [ ] Document quality metrics

### Day 4: Export & Integration
- [ ] Export embeddings in multiple formats (numpy, txt)
- [ ] Create embedding lookup API
- [ ] Build Redis integration for fast retrieval
- [ ] Add versioning and metadata
- [ ] Performance testing: lookup latency <1ms
- [ ] Documentation and examples

## âœ… Acceptance Criteria
- [ ] PPMI matrix successfully computed from co-occurrence
- [ ] SVD produces dense embeddings (100-300 dimensions)
- [ ] Intrinsic quality: top-10 neighbors are semantically coherent
- [ ] Cross-language semantic connections detected (â‰¥60% precision)
- [ ] Memory efficient: handles 1M+ vocabulary
- [ ] Embedding lookup latency: <1ms per query
- [ ] Embeddings improve retrieval recall by â‰¥15% in test set

## ðŸ§ª Testing Checklist
- [ ] Unit tests for PPMI computation (5+ test cases)
- [ ] SVD convergence tests
- [ ] Intrinsic evaluation suite (analogy, similarity)
- [ ] Cross-language semantic tests
- [ ] Performance benchmarks (lookup speed)
- [ ] Memory profiling tests
- [ ] Multi-language validation (10+ languages)

## ðŸŽ‰ Celebration Criteria (Definition of Done)
âœ… **Demo Ready:** Show semantic neighbors for query terms in 5 languages
âœ… **Metric Met:** Nearest neighbors are semantically coherent (manual inspection)
âœ… **Integration:** Embeddings successfully loaded into Redis with <1ms lookup
âœ… **Evaluation:** Retrieval recall improvement confirmed on test queries

**ðŸŽŠ Celebration Moment:** Discover interesting cross-language semantic connections!

## ðŸ“¦ Deliverables
- `src/python/embeddings/ppmi_trainer.py` (200-250 lines)
- `src/python/embeddings/svd_trainer.py` (150-200 lines)
- `src/python/embeddings/embedding_exporter.py` (100 lines)
- `tests/test_ppmi.py` (50+ test cases)
- `tests/test_svd_embeddings.py` (40+ test cases)
- `data/embeddings/ppmi_svd_v1.npz` (compressed embeddings)
- `docs/embeddings/ppmi-svd-guide.md`

## ðŸ”— Dependencies & Integration

### Input
```python
# From Task 05.1: Co-occurrence Matrix
{
    "cooccurrence_matrix": scipy.sparse.csr_matrix,  # (vocab_size, vocab_size)
    "word2idx": Dict[str, int],
    "idx2word": Dict[int, str],
    "word_counts": Dict[str, int]
}
```

### Output
```python
# PPMI embeddings
{
    "embeddings": np.ndarray,  # (vocab_size, embedding_dim)
    "word2idx": Dict[str, int],
    "metadata": {
        "dimensions": int,
        "vocabulary_size": int,
        "training_corpus_size": int,
        "version": str
    }
}
```

### External Dependencies
- NumPy 1.20+ (matrix operations)
- SciPy 1.7+ (sparse matrices, SVD)
- scikit-learn 1.0+ (TruncatedSVD)
- Redis-py 4.0+ (embedding storage)

## ðŸš€ Next Steps
âž¡ï¸ **Task 05.3: Subword Embeddings Training** (5 days)
- Complements PPMI/SVD with character-level representations
- Handles OOV words and typos

âž¡ï¸ **Task 05.4: Embedding Inference Service** (3 days)
- HTTP/gRPC service for embedding lookup
- Powers query expansion and re-ranking

## ðŸ’¡ Tips & Resources

### Common Pitfalls
- âš ï¸ **Memory explosion**: Use sparse matrices throughout pipeline
- âš ï¸ **Negative PPMI values**: Should clip at zero (hence "Positive" PMI)
- âš ï¸ **SVD slow convergence**: Use randomized SVD for large matrices
- âš ï¸ **Context bias**: Balance word frequency weighting

### Helpful Resources
- [Levy & Goldberg: Neural Word Embeddings as Implicit Matrix Factorization](https://papers.nips.cc/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf)
- [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)
- [SciPy Sparse Matrices](https://docs.scipy.org/doc/scipy/reference/sparse.html)
- [sklearn TruncatedSVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html)

### Example Code
```python
import numpy as np
from scipy.sparse import csr_matrix
from sklearn.decomposition import TruncatedSVD

def compute_ppmi(cooccur_matrix, word_counts, smoothing=0.75):
    """Compute PPMI from co-occurrence matrix"""
    # Smooth word counts
    word_counts_smooth = np.power(word_counts, smoothing)
    
    # Compute PMI
    pmi_matrix = np.log2(
        cooccur_matrix.multiply(word_counts_smooth.sum()) /
        (word_counts_smooth[:, None] @ word_counts_smooth[None, :])
    )
    
    # Clip negative values (Positive PMI)
    ppmi_matrix = pmi_matrix.maximum(0)
    
    return ppmi_matrix

def train_svd_embeddings(ppmi_matrix, n_components=300):
    """Train SVD embeddings from PPMI"""
    svd = TruncatedSVD(n_components=n_components, random_state=42)
    embeddings = svd.fit_transform(ppmi_matrix)
    
    return embeddings, svd
```

## ðŸ“Š Success Metrics
- **Quality:** Semantic coherence in top-10 neighbors â‰¥80%
- **Performance:** SVD training completes in <2 hours for 1M vocab
- **Coverage:** Embeddings for 100K+ vocabulary terms
- **Retrieval Impact:** Recall improvement â‰¥15% on test queries
- **Cross-lingual:** Semantic connections detected across languages

## ðŸŽ“ Learning Outcomes
After completing this task, you will:
- âœ… Understand distributional semantics and PMI
- âœ… Master sparse matrix operations for efficiency
- âœ… Know how to apply SVD for dimensionality reduction
- âœ… Evaluate embedding quality intrinsically
- âœ… Integrate embeddings into production systems

---

**Building the semantic foundation! ðŸ§ âœ¨**

