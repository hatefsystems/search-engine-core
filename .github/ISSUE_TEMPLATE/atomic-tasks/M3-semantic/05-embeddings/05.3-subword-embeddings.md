# üéØ Task 05.3: Subword Embeddings Training (FastText-style)

## üìÖ Sprint Info
- **Duration:** 5 days
- **Milestone:** M3 - Semantic Processing
- **Priority:** P1
- **Depends On:** Task 05.2 (PPMI/SVD Embeddings) ‚úÖ
- **Blocks:** Task 05.6 (Spell Correction Models)
- **Assignee:** TBD

## üé¨ What You'll Build
Train universal multilingual subword embeddings using FastText-style skip-gram models. Handle any language/script automatically, including OOV words, typos, and morphologically rich languages.

## üìã Daily Breakdown

### Day 1: Data Preparation & Subword Tokenization
- [ ] Build character n-gram extractor (3-6 grams)
- [ ] Implement subword tokenization for any script
- [ ] Create training data from corpus (titles + body + anchors)
- [ ] Add special tokens (<BOW>, <EOW> for word boundaries)
- [ ] Validate tokenization on 10+ languages
- [ ] Estimate vocabulary size and storage needs

### Day 2: Skip-Gram Model Architecture
- [ ] Implement skip-gram with negative sampling
- [ ] Build subword embedding layer
- [ ] Add context window sampling (5-10 words)
- [ ] Implement efficient negative sampling
- [ ] Create training batch generator
- [ ] Add progress tracking and logging

### Day 3: Training Pipeline
- [ ] Train on multilingual corpus (100M+ tokens)
- [ ] Implement learning rate scheduling
- [ ] Add checkpointing every N iterations
- [ ] Monitor training loss convergence
- [ ] Validate on held-out test set
- [ ] Tune hyperparameters (dim, window, negatives)

### Day 4: OOV Handling & Inference
- [ ] Implement OOV word representation (sum of n-grams)
- [ ] Build fast inference for unknown words
- [ ] Add typo-robust similarity computation
- [ ] Test on misspellings and rare words
- [ ] Benchmark inference speed (<5ms per word)
- [ ] Compare OOV coverage vs word-level embeddings

### Day 5: Export, Optimization & Integration
- [ ] Export embeddings in multiple formats
- [ ] Quantize embeddings for memory efficiency (optional)
- [ ] Build embedding lookup service API
- [ ] Add Redis caching for hot queries
- [ ] Performance testing and optimization
- [ ] Documentation and usage examples

## ‚úÖ Acceptance Criteria
- [ ] Subword embeddings trained on multilingual corpus (100M+ tokens)
- [ ] Works for any Unicode script without manual configuration
- [ ] OOV word representation quality: typos within edit distance 2 recognized
- [ ] Embedding dimension: 100-300 (configurable)
- [ ] Training completes within 24 hours on single GPU/8-core CPU
- [ ] Inference speed: <5ms per word embedding
- [ ] Memory efficient: vocabulary + embeddings <2GB
- [ ] Improves retrieval recall by ‚â•20% for rare/misspelled queries

## üß™ Testing Checklist
- [ ] Unit tests for subword tokenization (10+ scripts)
- [ ] Training convergence tests
- [ ] OOV word similarity tests
- [ ] Typo robustness evaluation (edit distance 1-2)
- [ ] Cross-language semantic tests
- [ ] Performance benchmarks (training, inference)
- [ ] Memory profiling
- [ ] Multi-language validation (20+ languages)

## üéâ Celebration Criteria (Definition of Done)
‚úÖ **Demo Ready:** Show similar words including typos and rare terms in 5 languages
‚úÖ **Metric Met:** OOV coverage ‚â•95%, typo detection ‚â•85%
‚úÖ **Integration:** Embeddings deployed to inference service with <5ms latency
‚úÖ **Evaluation:** Retrieval recall improvement confirmed for rare queries

**üéä Celebration Moment:** Successfully handle misspellings without explicit dictionary!

## üì¶ Deliverables
- `src/python/embeddings/subword_tokenizer.py` (150-200 lines)
- `src/python/embeddings/skipgram_trainer.py` (300-400 lines)
- `src/python/embeddings/fasttext_model.py` (200-250 lines)
- `tests/test_subword_embeddings.py` (60+ test cases)
- `tests/test_oov_handling.py` (40+ test cases)
- `models/fasttext_multilingual_v1.bin` (embedding model)
- `data/embeddings/subword_vocab.json` (subword vocabulary)
- `docs/embeddings/fasttext-training-guide.md`

## üîó Dependencies & Integration

### Input
```python
# Training corpus
{
    "documents": List[str],  # Multilingual text documents
    "vocabulary": Set[str],  # Word vocabulary (from text processing)
    "language_labels": List[str]  # Optional: language per document
}
```

### Output
```python
# Subword embeddings
{
    "word_embeddings": Dict[str, np.ndarray],  # Word -> vector
    "subword_embeddings": Dict[str, np.ndarray],  # n-gram -> vector
    "vocab_size": int,
    "embedding_dim": int,
    "model_version": str
}

# Inference API
def get_word_embedding(word: str) -> np.ndarray:
    """Get embedding for any word (including OOV)"""
    pass

def find_similar_words(word: str, top_k: int = 10) -> List[Tuple[str, float]]:
    """Find semantically similar words"""
    pass
```

### External Dependencies
- Gensim 4.0+ (FastText implementation)
- NumPy 1.20+ (array operations)
- Cython (optional, for speedup)
- Redis-py 4.0+ (embedding cache)

## üöÄ Next Steps
‚û°Ô∏è **Task 05.4: Embedding Inference Service** (3 days)
- HTTP/gRPC service wrapping trained embeddings
- Powers query expansion and semantic re-ranking

‚û°Ô∏è **Task 05.6: Spell Correction Models** (4 days)
- Uses subword embeddings for typo detection
- Character n-gram models for edit distance candidates

## üí° Tips & Resources

### Common Pitfalls
- ‚ö†Ô∏è **Subword vocabulary explosion**: Limit n-gram length to 3-6 characters
- ‚ö†Ô∏è **Training time**: Use hierarchical softmax or negative sampling (not full softmax)
- ‚ö†Ô∏è **Memory issues**: Stream training data, don't load all into RAM
- ‚ö†Ô∏è **Script mixing**: Ensure proper Unicode handling for all scripts
- ‚ö†Ô∏è **Rare n-grams**: Apply min-count threshold to reduce noise

### Helpful Resources
- [FastText: Enriching Word Vectors with Subword Information](https://arxiv.org/abs/1607.04606)
- [Gensim FastText Tutorial](https://radimrehurek.com/gensim/models/fasttext.html)
- [Facebook FastText Official](https://fasttext.cc/)
- [Subword Tokenization Explained](https://huggingface.co/docs/transformers/tokenizer_summary)

### Example Code
```python
from gensim.models import FastText
from gensim.models.callbacks import CallbackAny2Vec

class TrainingCallback(CallbackAny2Vec):
    def on_epoch_end(self, model):
        print(f"Epoch {model.epochs_iterated}: Loss = {model.get_latest_training_loss()}")

def train_fasttext_embeddings(sentences, vector_size=300, window=5, min_count=5, epochs=5):
    """Train FastText subword embeddings"""
    model = FastText(
        sentences=sentences,
        vector_size=vector_size,
        window=window,
        min_count=min_count,
        min_n=3,  # Min n-gram length
        max_n=6,  # Max n-gram length
        sg=1,     # Skip-gram
        workers=8,
        epochs=epochs,
        callbacks=[TrainingCallback()]
    )
    
    return model

# OOV handling example
def get_word_vector(model, word):
    """Get vector for word (even if OOV)"""
    try:
        return model.wv[word]
    except KeyError:
        # FastText automatically handles OOV via subword composition
        return model.wv.get_vector(word, norm=True)

# Find similar words (including typos)
similar_words = model.wv.most_similar("searhc")  # Typo: "search"
# Returns: [("search", 0.89), ("searching", 0.85), ...]
```

## üìä Success Metrics
- **Quality:** Semantic similarity on test pairs ‚â•0.7 correlation
- **Performance:** Training completes <24h for 100M tokens
- **Coverage:** OOV word representation accuracy ‚â•85%
- **Robustness:** Typo detection (edit distance 1-2) accuracy ‚â•85%
- **Retrieval Impact:** Recall improvement ‚â•20% for rare queries
- **Memory:** Model size <2GB for 1M vocabulary

## üéì Learning Outcomes
After completing this task, you will:
- ‚úÖ Understand subword segmentation and character n-grams
- ‚úÖ Master skip-gram architecture and negative sampling
- ‚úÖ Train large-scale multilingual embeddings
- ‚úÖ Handle OOV words gracefully with subword composition
- ‚úÖ Optimize neural models for production deployment

---

**Subword magic: Never say "unknown word" again! ‚ú®üî§**

