
services:
  # zookeeper:
  #   image: bitnami/zookeeper:3.9
  #   container_name: zookeeper
  #   restart: unless-stopped
  #   environment:
  #     - ALLOW_ANONYMOUS_LOGIN=yes
  #   ports:
  #     - "2181:2181"
  #   networks:
  #     - search-network

  # kafka:
  #   image: bitnami/kafka:3.7
  #   container_name: kafka
  #   restart: unless-stopped
  #   depends_on:
  #     - zookeeper
  #   environment:
  #     - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
  #     - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092
  #     - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
  #     - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
  #     - ALLOW_PLAINTEXT_LISTENER=yes
  #   ports:
  #     - "9092:9092"
  #   healthcheck:
  #     test: ["CMD", "bash", "-c", "kafka-topics.sh --bootstrap-server localhost:9092 --list || exit 1"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 10
  #   networks:
  #     - search-network
  search-engine:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: core
    restart: unless-stopped
    ports:
      - "3000:3000"
    env_file:
    - .env
    environment:
      - LOG_LEVEL=${LOG_LEVEL:-info}  # CRITICAL: Configurable logging (debug, info, warning, error, none)
      - BASE_URL=${BASE_URL:-http://localhost:3000}  # Base URL for internal API calls
      - MONGODB_URI=mongodb://admin:password123@mongodb:27017
      - REDIS_URL=tcp://redis:6379
      - SEARCH_REDIS_URI=tcp://redis:6379
      - SEARCH_REDIS_POOL_SIZE=4
      - SEARCH_INDEX_NAME=search_index
      - REDIS_SEARCH_ENABLED=${REDIS_SEARCH_ENABLED:-true}  # Enable Redis-based search (10-100x faster than MongoDB)
      - MINIFY_JS=true
      - MINIFY_JS_LEVEL=none  # Use microservice instead
      - JS_MINIFIER_SERVICE_URL=http://js-minifier:3002
      # Cache configuration
      - JS_CACHE_ENABLED=true
      - JS_CACHE_TYPE=redis
      - JS_CACHE_TTL=3600
      - JS_CACHE_REDIS_DB=1
      # - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      # - KAFKA_FRONTIER_TOPIC=crawl.frontier
      # Crawler configuration
      - MAX_CONCURRENT_SESSIONS=${MAX_CONCURRENT_SESSIONS:-5}  # Maximum concurrent crawler sessions
      - SPA_RENDERING_ENABLED=${SPA_RENDERING_ENABLED:-true}
      - SPA_RENDERING_TIMEOUT=${SPA_RENDERING_TIMEOUT:-60000}
      - BROWSERLESS_URL=${BROWSERLESS_URL:-http://browserless:3000}
      - DEFAULT_REQUEST_TIMEOUT=${DEFAULT_REQUEST_TIMEOUT:-60000}
      # SMTP Email Configuration
      - SMTP_HOST=${SMTP_HOST:-smtp.gmail.com}
      - SMTP_PORT=${SMTP_PORT:-587}
      - SMTP_USE_TLS=${SMTP_USE_TLS:-true}
      - SMTP_USE_SSL=${SMTP_USE_SSL:-false}
      - SMTP_TIMEOUT=${SMTP_TIMEOUT:-60}
      - SMTP_CONNECTION_TIMEOUT=${SMTP_CONNECTION_TIMEOUT:-20}
      - SMTP_USERNAME=${SMTP_USERNAME}
      - SMTP_PASSWORD=${SMTP_PASSWORD}
      - FROM_EMAIL=${FROM_EMAIL:-noreply@hatef.ir}
      - FROM_NAME=${FROM_NAME:-Hatef.ir Search Engine}
      - EMAIL_SERVICE_ENABLED=${EMAIL_SERVICE_ENABLED:-true}
      - EMAIL_ASYNC_ENABLED=${EMAIL_ASYNC_ENABLED:-false}
    depends_on:
      redis:
        condition: service_healthy
      mongodb:
        condition: service_healthy
      js-minifier:
        condition: service_healthy
      # - kafka
    networks:
      - search-network
    dns:
      - 8.8.8.8
      - 1.1.1.1
      - 8.8.4.4

  js-minifier:
    build:
      context: ./js-minifier-service
      dockerfile: Dockerfile
    container_name: js-minifier
    restart: unless-stopped
    pull_policy: if_not_present
    ports:
      - "3002:3002"
    environment:
      - NODE_ENV=production
      - PORT=3002
    networks:
      - search-network
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3002/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  redis:
    image: redis/redis-stack:7.2.0-v7
    container_name: redis
    restart: unless-stopped
    ports:
      - "6379:6379"
      - "8001:8001"  # RedisInsight web UI
    volumes:
      - redis_data:/data
    # Development: Less aggressive save policy for stability
    command: [
      "redis-stack-server",
      "--appendonly", "yes",
      "--appendfsync", "everysec",
      "--maxmemory", "3gb",
      "--maxmemory-policy", "allkeys-lfu",
      "--save", "3600", "1",  # Save every 1 hour if 1+ keys changed
      "--save", "1800", "100",  # Save every 30 min if 100+ keys changed
      "--save", "900", "10000",  # Save every 15 min if 10k+ keys changed
      "--stop-writes-on-bgsave-error", "no"  # Don't stop writes if background save fails
    ]
    environment:
      - REDIS_MAXMEMORY=3221225472  # 3GB in bytes (corrected from 3221225476)
      - REDIS_MAXMEMORY_POLICY=allkeys-lfu
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    # System-level settings for Redis fork operations
    sysctls:
      - net.core.somaxconn=65535
      # vm.overcommit_memory=1 removed for WSL2 compatibility
    stop_grace_period: 60s  # Allow Redis time to finish background saves on shutdown
    networks:
      - search-network

  mongodb:
    image: mongo:7
    container_name: mongodb_test
    restart: unless-stopped
    ports:
      - "27017:27017"
    volumes:
      - mongodb_data:/data/db
    environment:
      - MONGO_INITDB_ROOT_USERNAME=admin
      - MONGO_INITDB_ROOT_PASSWORD=password123
    command: ["mongod", "--bind_ip_all", "--wiredTigerCacheSizeGB", "1.0", "--maxConns", "100"]
    healthcheck:
      test: ["CMD", "mongosh", "--eval", "db.runCommand('ping')"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - search-network

  browserless:
    image: browserless/chrome:latest
    container_name: browserless
    restart: unless-stopped
    ports:
      - "3001:3000"
    environment:
      - "MAX_CONCURRENT_SESSIONS=10"
      - "PREBOOT_CHROME=true"
      - "CONNECTION_TIMEOUT=15000"
      - "CHROME_REFRESH_TIME=60000"
      - "QUEUE_LIMIT=100"
      - "MAX_CPU_PERCENT=90"
      - "MAX_MEMORY_PERCENT=90"
      - "KEEP_ALIVE=true"
      - "ENABLE_DEBUGGER=false"
      - "ENABLE_CORS=true"
      - "WORKSPACE_DIR=/workspace"
      - "FUNCTION_ENABLE_INCOGNITO=false"
      - "FUNCTION_KEEP_ALIVE=true"
    networks:
      - search-network
    dns:
      - 8.8.8.8
      - 1.1.1.1
      - 8.8.4.4
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G

  # mongodb-test:
  #   build:
  #     context: .
  #     dockerfile: Dockerfile.test.mongo
  #   depends_on:
  #     - mongodb
  #   environment:
  #     - MONGODB_URI=mongodb://mongodb:27017

  # Crawler Scheduler - Progressive Warm-up Task Scheduler
  crawler-scheduler:
    build: ./crawler-scheduler
    container_name: crawler-scheduler-worker
    restart: unless-stopped
    command: celery -A app.celery_app worker --beat --loglevel=info
    volumes:
      - ./crawler-scheduler/data:/app/data
      - ./crawler-scheduler/app:/app/app  # Hot reload for development
    environment:
      # Celery Configuration
      - CELERY_BROKER_URL=redis://redis:6379/2
      - CELERY_RESULT_BACKEND=redis://redis:6379/2
      
      # MongoDB Configuration
      - MONGODB_URI=mongodb://admin:password123@mongodb_test:27017
      - MONGODB_DB=search-engine
      
      # API Configuration
      - API_BASE_URL=http://core:3000
      
      # Timezone Configuration (Auto-detects system timezone by default)
      - TZ=Asia/Tehran  # System timezone for Celery worker
      - SCHEDULER_TIMEZONE=${SCHEDULER_TIMEZONE:-Asia/Tehran}  # Optional: Override system timezone (e.g., America/New_York, Europe/London)
      
      # Warm-up Configuration (Progressive Rate Limiting)
      - WARMUP_ENABLED=${CRAWLER_WARMUP_ENABLED:-true}
      - WARMUP_SCHEDULE=${CRAWLER_WARMUP_SCHEDULE:-50,100,200,400,800}  # Day 1: 50, Day 2: 100, etc.
      - WARMUP_START_HOUR=${CRAWLER_WARMUP_START_HOUR:-0}   # Start hour in configured timezone (0-23)
      - WARMUP_END_HOUR=${CRAWLER_WARMUP_END_HOUR:-23}       # End hour in configured timezone (inclusive, 0-23)
      
      # Jitter Configuration (Randomization to avoid exact timing)
      - JITTER_MIN_SECONDS=${CRAWLER_JITTER_MIN:-30}
      - JITTER_MAX_SECONDS=${CRAWLER_JITTER_MAX:-60}
      
      # Task Configuration
      - TASK_INTERVAL_SECONDS=${CRAWLER_TASK_INTERVAL:-60}  # Check for new files every 60 seconds
      - MAX_RETRIES=${CRAWLER_MAX_RETRIES:-3}
      - RETRY_DELAY_SECONDS=${CRAWLER_RETRY_DELAY:-300}
      
      # Logging
      - LOG_LEVEL=${LOG_LEVEL:-info}
    networks:
      - search-network
    depends_on:
      - redis
      - mongodb
      - search-engine
  
  # Flower Web UI - Scheduler Monitoring Dashboard
  crawler-flower:
    build: ./crawler-scheduler
    container_name: crawler-scheduler-flower
    restart: unless-stopped
    command: celery -A app.celery_app flower --port=5555
    ports:
      - "5555:5555"
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/2
      - CELERY_RESULT_BACKEND=redis://redis:6379/2
      - FLOWER_BASIC_AUTH=${FLOWER_BASIC_AUTH:-admin:admin123}
      # Timezone Configuration for Flower Dashboard
      - TZ=Asia/Tehran
      - SCHEDULER_TIMEZONE=${SCHEDULER_TIMEZONE:-Asia/Tehran}
    networks:
      - search-network
    depends_on:
      - redis
      - crawler-scheduler

  # Redis Sync Service - Syncs MongoDB indexed_pages to Redis for fast search
  redis-sync:
    build: ./redis-sync-service
    container_name: redis-sync
    restart: unless-stopped
    environment:
      - MONGODB_URI=mongodb://admin:password123@mongodb:27017
      - MONGODB_DB=search-engine
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=0
      - SEARCH_INDEX_NAME=search_index
      - "KEY_PREFIX=doc:"
      - SYNC_MODE=${REDIS_SYNC_MODE:-incremental}
      - SYNC_INTERVAL_SECONDS=${REDIS_SYNC_INTERVAL:-3600}
      - INCREMENTAL_WINDOW_HOURS=${REDIS_INCREMENTAL_WINDOW:-24}
      - BATCH_SIZE=${REDIS_SYNC_BATCH_SIZE:-100}
      - MAX_CONTENT_SIZE=300
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    networks:
      - search-network
    depends_on:
      - redis
      - mongodb

networks:
  search-network:
    driver: bridge

volumes:
  mongodb_data:
  redis_data: 